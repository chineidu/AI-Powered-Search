{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.9\n",
      "IPython version      : 8.26.0\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.2\n",
      "polars   : 1.0.0\n",
      "torch    : 2.2.2\n",
      "lightning: 2.3.2\n",
      "\n",
      "conda environment: ai_search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'happy': 1, 'i': 2, 'today': 3, 'very': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text: list[str] = [\"i\", \"am\", \"very\", \"happy\", \"today\"]\n",
    "vocab_set: set = sorted(set(text))\n",
    "vocab: dict[str, int] = {word: idx for idx, word in enumerate(vocab_set, start=0)}\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'very', 'happy', 'today']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern: str = r'([,.?_!\"()\\']|--|\\s)'\n",
    "result: list[str] = re.split(pattern=pattern, string=\" \".join(text))\n",
    "result = [x for x in result if x.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def tokenize(doc: str | list[str], drop_punct: bool = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the input document and optionally remove punctuation.\n",
    "\n",
    "    Args:\n",
    "        doc (str | list[str]): The input document as a string or list of strings.\n",
    "        drop_punct (bool, optional): Whether to remove punctuation. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The tokenized document as a list of strings.\n",
    "    \"\"\"\n",
    "    # Pattern for separating tokens\n",
    "    pattern: str = r'([,.?_!\"()\\':]|\\s)'\n",
    "\n",
    "    if isinstance(doc, str):\n",
    "        tok_doc: list[str] = re.split(pattern=pattern, string=doc.lower())\n",
    "\n",
    "    if isinstance(doc, list):\n",
    "        doc = [word.lower() for word in doc]\n",
    "        tok_doc = re.split(pattern=pattern, string=\" \".join(doc))\n",
    "\n",
    "    # Remove whitespaces and empty strings\n",
    "    tok_doc = [word for word in tok_doc if word.strip()]\n",
    "\n",
    "    if drop_punct:\n",
    "        tok_doc = [word for word in tok_doc if word not in string.punctuation]\n",
    "\n",
    "    return tok_doc\n",
    "\n",
    "\n",
    "def flatten_documents(docs: list[list[str]]) -> list[str]:\n",
    "\n",
    "    assert all(\n",
    "        [True if isinstance(row, list) else False for row in docs]\n",
    "    ), \"Not all elements are lists\"\n",
    "\n",
    "    flattened_doc: list[str] = [word.lower() for row in docs for word in row]\n",
    "    return flattened_doc\n",
    "\n",
    "\n",
    "def generate_vocab(doc: list[list[str]], drop_punct: bool = True) -> dict[str, int]:\n",
    "\n",
    "    flattened_doc: list[str] = flatten_documents(doc)\n",
    "    tok_doc: list[str] = tokenize(flattened_doc)\n",
    "    tok_doc = sorted(set(tok_doc))\n",
    "\n",
    "    vocab: dict[str, int] = {word: idx for idx, word in enumerate(tok_doc, start=0)}\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def encode(doc: list[list[str]], vocab: dict[str, int]) -> list[int]:\n",
    "    \"\"\"This particular implementation checks for the occurrence of a term.\"\"\"\n",
    "    arr: np.ndarray = np.zeros((1, len(vocab)), dtype=int)\n",
    "    for row in doc:\n",
    "        for word in tokenize(row):\n",
    "            if word in vocab:\n",
    "                arr[0, vocab[word]] = 1  # change!\n",
    "    return arr\n",
    "\n",
    "\n",
    "def encode_n_create_df(doc: list[list[str]], vocab: dict[str, int]) -> pl.DataFrame:\n",
    "    df: pl.DataFrame = pl.DataFrame(encode(doc=doc, vocab=vocab))\n",
    "    df.columns = list(vocab.keys())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6\n",
      "vocab = {'about': 0, 'hey': 1, 'me': 2, 'neidu': 3, 'something': 4, 'tell': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc: list[list[str]] = [[\"Hey! Tell me something about neidu.\"]]\n",
    "vocab: dict[str, int] = generate_vocab(doc=doc, drop_punct=True)\n",
    "print(f\"{vocab = }\")\n",
    "\n",
    "encode(doc=[[\"tell Tell\"]], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>about</th><th>hey</th><th>me</th><th>neidu</th><th>something</th><th>tell</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌───────┬─────┬─────┬───────┬───────────┬──────┐\n",
       "│ about ┆ hey ┆ me  ┆ neidu ┆ something ┆ tell │\n",
       "│ ---   ┆ --- ┆ --- ┆ ---   ┆ ---       ┆ ---  │\n",
       "│ i64   ┆ i64 ┆ i64 ┆ i64   ┆ i64       ┆ i64  │\n",
       "╞═══════╪═════╪═════╪═══════╪═══════════╪══════╡\n",
       "│ 1     ┆ 0   ┆ 0   ┆ 0     ┆ 0         ┆ 1    │\n",
       "└───────┴─────┴─────┴───────┴───────────┴──────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_n_create_df(doc=[[\"about Tell\"]], vocab=vocab)\n",
    "\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "#### [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity#:~:text=Cosine%20similarity%20is%20the%20cosine,but%20only%20on%20their%20angle.)\n",
    "\n",
    "- Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths.\n",
    "- It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle.\n",
    "\n",
    "$$cosSimilarity = cos(\\theta) = \\frac{A . B}{||A||.||B||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray) -> float:\n",
    "    return np.dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
    "\n",
    "\n",
    "def check_equality(vector_1: np.ndarray, vector_2: np.ndarray) -> bool:\n",
    "    result: bool = np.array_equal(vector_1, vector_2)\n",
    "    print(f\"{result = }\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746318461970762"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector1=np.array([1, 2, 3]), vector2=np.array([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 48\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "doc1: list[str] = [\n",
    "    (\n",
    "        \"Lynn: ham and cheese sandwich, chocolate cookie, ice water. \"\n",
    "        \"Brian: turkey avocado sandwich, plain potato chips, apple juice \"\n",
    "        \"Mohammed: grilled chicken salad, fruit cup, lemonade \"\n",
    "    )\n",
    "]\n",
    "\n",
    "doc2: list[str] = [\n",
    "    (\n",
    "        \"Orchard Farms apple juice is premium, organic apple juice made from the \"\n",
    "        \"freshest apples, never from concentrate. Its juice has received the \"\n",
    "        \"regional award for best apple juice three years in a row. \"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "doc: list[list[str]] = [doc1, doc2]\n",
    "vocab: dict[str, int] = generate_vocab(doc=doc)\n",
    "\n",
    "query: list[list[str]] = [[\" apple juice\"]]\n",
    "query_vector: np.ndarray = encode(doc=query, vocab=vocab)\n",
    "doc1_vector: np.ndarray = encode(doc=[doc1], vocab=vocab)\n",
    "doc2_vector: np.ndarray = encode(doc=[doc2], vocab=vocab)\n",
    "\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'apple'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'apples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'avocado'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'award'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'best'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'brian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cheese'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chicken'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chips'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chocolate'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'concentrate'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cookie'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cup'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'farms'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'for'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'freshest'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'from'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'fruit'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'grilled'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ham'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'has'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ice'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'its'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'juice'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lemonade'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lynn'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'made'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'mohammed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'never'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'orchard'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'organic'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'plain'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'potato'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'premium'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'received'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'regional'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'row'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'salad'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sandwich'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'three'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'turkey'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'water'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'years'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'a'\u001b[0m,\n",
       "    \u001b[32m'and'\u001b[0m,\n",
       "    \u001b[32m'apple'\u001b[0m,\n",
       "    \u001b[32m'apples'\u001b[0m,\n",
       "    \u001b[32m'avocado'\u001b[0m,\n",
       "    \u001b[32m'award'\u001b[0m,\n",
       "    \u001b[32m'best'\u001b[0m,\n",
       "    \u001b[32m'brian'\u001b[0m,\n",
       "    \u001b[32m'cheese'\u001b[0m,\n",
       "    \u001b[32m'chicken'\u001b[0m,\n",
       "    \u001b[32m'chips'\u001b[0m,\n",
       "    \u001b[32m'chocolate'\u001b[0m,\n",
       "    \u001b[32m'concentrate'\u001b[0m,\n",
       "    \u001b[32m'cookie'\u001b[0m,\n",
       "    \u001b[32m'cup'\u001b[0m,\n",
       "    \u001b[32m'farms'\u001b[0m,\n",
       "    \u001b[32m'for'\u001b[0m,\n",
       "    \u001b[32m'freshest'\u001b[0m,\n",
       "    \u001b[32m'from'\u001b[0m,\n",
       "    \u001b[32m'fruit'\u001b[0m,\n",
       "    \u001b[32m'grilled'\u001b[0m,\n",
       "    \u001b[32m'ham'\u001b[0m,\n",
       "    \u001b[32m'has'\u001b[0m,\n",
       "    \u001b[32m'ice'\u001b[0m,\n",
       "    \u001b[32m'in'\u001b[0m,\n",
       "    \u001b[32m'is'\u001b[0m,\n",
       "    \u001b[32m'its'\u001b[0m,\n",
       "    \u001b[32m'juice'\u001b[0m,\n",
       "    \u001b[32m'lemonade'\u001b[0m,\n",
       "    \u001b[32m'lynn'\u001b[0m,\n",
       "    \u001b[32m'made'\u001b[0m,\n",
       "    \u001b[32m'mohammed'\u001b[0m,\n",
       "    \u001b[32m'never'\u001b[0m,\n",
       "    \u001b[32m'orchard'\u001b[0m,\n",
       "    \u001b[32m'organic'\u001b[0m,\n",
       "    \u001b[32m'plain'\u001b[0m,\n",
       "    \u001b[32m'potato'\u001b[0m,\n",
       "    \u001b[32m'premium'\u001b[0m,\n",
       "    \u001b[32m'received'\u001b[0m,\n",
       "    \u001b[32m'regional'\u001b[0m,\n",
       "    \u001b[32m'row'\u001b[0m,\n",
       "    \u001b[32m'salad'\u001b[0m,\n",
       "    \u001b[32m'sandwich'\u001b[0m,\n",
       "    \u001b[32m'the'\u001b[0m,\n",
       "    \u001b[32m'three'\u001b[0m,\n",
       "    \u001b[32m'turkey'\u001b[0m,\n",
       "    \u001b[32m'water'\u001b[0m,\n",
       "    \u001b[32m'years'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2886751345948129, 0.2773500981126146)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_score = cosine_similarity(query_vector.squeeze(), doc1_vector.squeeze())\n",
    "doc2_score = cosine_similarity(query_vector.squeeze(), doc2_vector.squeeze())\n",
    "\n",
    "doc1_score, doc2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 48)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>and</th><th>apple</th><th>apples</th><th>avocado</th><th>award</th><th>best</th><th>brian</th><th>cheese</th><th>chicken</th><th>chips</th><th>chocolate</th><th>concentrate</th><th>cookie</th><th>cup</th><th>farms</th><th>for</th><th>freshest</th><th>from</th><th>fruit</th><th>grilled</th><th>ham</th><th>has</th><th>ice</th><th>in</th><th>is</th><th>its</th><th>juice</th><th>lemonade</th><th>lynn</th><th>made</th><th>mohammed</th><th>never</th><th>orchard</th><th>organic</th><th>plain</th><th>potato</th><th>premium</th><th>received</th><th>regional</th><th>row</th><th>salad</th><th>sandwich</th><th>the</th><th>three</th><th>turkey</th><th>water</th><th>years</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 48)\n",
       "┌─────┬─────┬───────┬────────┬───┬───────┬────────┬───────┬───────┐\n",
       "│ a   ┆ and ┆ apple ┆ apples ┆ … ┆ three ┆ turkey ┆ water ┆ years │\n",
       "│ --- ┆ --- ┆ ---   ┆ ---    ┆   ┆ ---   ┆ ---    ┆ ---   ┆ ---   │\n",
       "│ i64 ┆ i64 ┆ i64   ┆ i64    ┆   ┆ i64   ┆ i64    ┆ i64   ┆ i64   │\n",
       "╞═════╪═════╪═══════╪════════╪═══╪═══════╪════════╪═══════╪═══════╡\n",
       "│ 0   ┆ 1   ┆ 1     ┆ 0      ┆ … ┆ 0     ┆ 1      ┆ 1     ┆ 0     │\n",
       "└─────┴─────┴───────┴────────┴───┴───────┴────────┴───────┴───────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_n_create_df(doc=[doc1], vocab=vocab)\n",
    "# encode(doc=[doc1], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 48)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>and</th><th>apple</th><th>apples</th><th>avocado</th><th>award</th><th>best</th><th>brian</th><th>cheese</th><th>chicken</th><th>chips</th><th>chocolate</th><th>concentrate</th><th>cookie</th><th>cup</th><th>farms</th><th>for</th><th>freshest</th><th>from</th><th>fruit</th><th>grilled</th><th>ham</th><th>has</th><th>ice</th><th>in</th><th>is</th><th>its</th><th>juice</th><th>lemonade</th><th>lynn</th><th>made</th><th>mohammed</th><th>never</th><th>orchard</th><th>organic</th><th>plain</th><th>potato</th><th>premium</th><th>received</th><th>regional</th><th>row</th><th>salad</th><th>sandwich</th><th>the</th><th>three</th><th>turkey</th><th>water</th><th>years</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 48)\n",
       "┌─────┬─────┬───────┬────────┬───┬───────┬────────┬───────┬───────┐\n",
       "│ a   ┆ and ┆ apple ┆ apples ┆ … ┆ three ┆ turkey ┆ water ┆ years │\n",
       "│ --- ┆ --- ┆ ---   ┆ ---    ┆   ┆ ---   ┆ ---    ┆ ---   ┆ ---   │\n",
       "│ i64 ┆ i64 ┆ i64   ┆ i64    ┆   ┆ i64   ┆ i64    ┆ i64   ┆ i64   │\n",
       "╞═════╪═════╪═══════╪════════╪═══╪═══════╪════════╪═══════╪═══════╡\n",
       "│ 1   ┆ 0   ┆ 1     ┆ 1      ┆ … ┆ 1     ┆ 0      ┆ 0     ┆ 1     │\n",
       "└─────┴─────┴───────┴────────┴───┴───────┴────────┴───────┴───────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_n_create_df(doc=[doc2], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = True\n",
      "result = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality(doc1_vector.squeeze(), doc1_vector.squeeze())\n",
    "check_equality(doc1_vector.squeeze(), doc2_vector.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### [Term Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "- Term frequency, tf(t,d), is the relative frequency of term t within document d.\n",
    "- where $f_{t,d}$ is the number of times term t appears in document d and $\\sum_{t' \\in d} f_{t',d}$ is the total number of terms in document d.\n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 56\n",
      "vocab = {'a': 0, 'achievements': 1, 'across': 2, 'all': 3, 'am': 4, 'and': 5, 'beautiful': 6, 'bedrock': 7, 'builds': 8, 'cat': 9, 'chased': 10, 'consistency': 11, 'creating': 12, 'day': 13, 'definitely': 14, 'dog': 15, 'drives': 16, 'enabling': 17, 'endeavors': 18, 'excellence': 19, 'for': 20, 'fosters': 21, 'foundation': 22, 'from': 23, 'gift': 24, 'grateful': 25, 'growth': 26, 'habits': 27, 'i': 28, 'improvement': 29, 'in': 30, 'is': 31, 'it': 32, 'jesus': 33, 'life': 34, 'log': 35, 'mat': 36, 'mouse': 37, 'myriad': 38, 'of': 39, 'on': 40, 'personal': 41, 'professional': 42, 'progress': 43, 'pursuits': 44, 'reliability': 45, 's': 46, 'sat': 47, 'steady': 48, 'success': 49, 'thank': 50, 'the': 51, 'to': 52, 'today': 53, 'trust': 54, 'you': 55}\n",
      "\n",
      "corpus = [['thank', 'you', 'jesus', 'for', 'the', 'gift', 'of', 'today', 'i', 'am', 'grateful', 'it', 'is', 'a', 'definitely', 'a', 'beautiful', 'day'], ['consistency', 'the', 'bedrock', 'of', 'progress', 'and', 'reliability', 'fosters', 'trust', 'builds', 'habits', 'and', 'drives', 'success', 'across', 'all', 'endeavors', 'from', 'personal', 'growth', 'to', 'professional', 'achievements', 'enabling', 'steady', 'improvement', 'and', 'creating', 'a', 'foundation', 'for', 'excellence', 'in', 'life', 's', 'myriad', 'pursuits'], ['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on', 'the', 'log', 'and', 'the', 'cat', 'chased', 'the', 'mouse', 'and', 'the', 'dog', 'chased', 'the', 'cat']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc1: list[str] = [\n",
    "    (\n",
    "        \"Thank you Jesus for the gift of today. I am grateful. \"\n",
    "        \"It is a definitely a beautiful day.\"\n",
    "    )\n",
    "]\n",
    "doc2: list[str] = [\n",
    "    (\n",
    "        \"Consistency, the bedrock of progress and reliability, fosters trust, \"\n",
    "        \"builds habits, and drives success across all endeavors, from personal \"\n",
    "        \"growth to professional achievements, enabling steady improvement and \"\n",
    "        \"creating a foundation for excellence in life's myriad pursuits.\"\n",
    "    )\n",
    "]\n",
    "doc3: list[str] = [\n",
    "    (\n",
    "        \"the cat sat on the mat the dog sat on the log \"\n",
    "        \"and the cat chased the mouse and the dog chased the cat\"\n",
    "    )\n",
    "]\n",
    "corpus: list[list[str]] = [tokenize(doc1), tokenize(doc2), tokenize(doc3)]\n",
    "vocab: dict[str, int] = generate_vocab(doc=corpus)\n",
    "print(f\"{vocab = }\\n\")\n",
    "print(f\"{corpus = }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.        , 0.        , 0.        , 0.05555556,\n",
       "        0.        , 0.05555556, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05555556, 0.05555556,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05555556, 0.        , 0.        , 0.        , 0.05555556,\n",
       "        0.05555556, 0.        , 0.        , 0.05555556, 0.        ,\n",
       "        0.        , 0.05555556, 0.05555556, 0.05555556, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05555556,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05555556, 0.05555556, 0.        , 0.05555556, 0.        ,\n",
       "        0.05555556],\n",
       "       [0.02702703, 0.02702703, 0.02702703, 0.02702703, 0.        ,\n",
       "        0.08108108, 0.        , 0.02702703, 0.02702703, 0.        ,\n",
       "        0.        , 0.02702703, 0.02702703, 0.        , 0.        ,\n",
       "        0.        , 0.02702703, 0.02702703, 0.02702703, 0.02702703,\n",
       "        0.02702703, 0.02702703, 0.02702703, 0.02702703, 0.        ,\n",
       "        0.        , 0.02702703, 0.02702703, 0.        , 0.02702703,\n",
       "        0.02702703, 0.        , 0.        , 0.        , 0.02702703,\n",
       "        0.        , 0.        , 0.        , 0.02702703, 0.02702703,\n",
       "        0.        , 0.02702703, 0.02702703, 0.02702703, 0.02702703,\n",
       "        0.02702703, 0.02702703, 0.        , 0.02702703, 0.02702703,\n",
       "        0.        , 0.02702703, 0.02702703, 0.        , 0.02702703,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.125     ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08333333, 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (n_docs, n_terms)\n",
    "freq_count: np.ndarray = np.zeros((len(corpus), len(vocab)), dtype=np.int32)\n",
    "\n",
    "for idx, doc in enumerate(corpus, start=0):\n",
    "    for word in doc:\n",
    "        word_idx = vocab[word]\n",
    "        freq_count[idx, word_idx] += 1\n",
    "\n",
    "\n",
    "term_freq: np.ndarray = freq_count / np.sum(freq_count, axis=1, keepdims=True)\n",
    "term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 0, 3, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 10,\n",
       "         'and': 5,\n",
       "         'a': 3,\n",
       "         'cat': 3,\n",
       "         'for': 2,\n",
       "         'of': 2,\n",
       "         'sat': 2,\n",
       "         'on': 2,\n",
       "         'dog': 2,\n",
       "         'chased': 2,\n",
       "         'thank': 1,\n",
       "         'you': 1,\n",
       "         'jesus': 1,\n",
       "         'gift': 1,\n",
       "         'today': 1,\n",
       "         'i': 1,\n",
       "         'am': 1,\n",
       "         'grateful': 1,\n",
       "         'it': 1,\n",
       "         'is': 1,\n",
       "         'definitely': 1,\n",
       "         'beautiful': 1,\n",
       "         'day': 1,\n",
       "         'consistency': 1,\n",
       "         'bedrock': 1,\n",
       "         'progress': 1,\n",
       "         'reliability': 1,\n",
       "         'fosters': 1,\n",
       "         'trust': 1,\n",
       "         'builds': 1,\n",
       "         'habits': 1,\n",
       "         'drives': 1,\n",
       "         'success': 1,\n",
       "         'across': 1,\n",
       "         'all': 1,\n",
       "         'endeavors': 1,\n",
       "         'from': 1,\n",
       "         'personal': 1,\n",
       "         'growth': 1,\n",
       "         'to': 1,\n",
       "         'professional': 1,\n",
       "         'achievements': 1,\n",
       "         'enabling': 1,\n",
       "         'steady': 1,\n",
       "         'improvement': 1,\n",
       "         'creating': 1,\n",
       "         'foundation': 1,\n",
       "         'excellence': 1,\n",
       "         'in': 1,\n",
       "         'life': 1,\n",
       "         's': 1,\n",
       "         'myriad': 1,\n",
       "         'pursuits': 1,\n",
       "         'mat': 1,\n",
       "         'log': 1,\n",
       "         'mouse': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "counter: dict = Counter(flatten_documents(corpus))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 56)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>2</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>3</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 56)\n",
       "┌─────┬──────────────┬────────┬─────┬───┬─────┬───────┬───────┬─────┐\n",
       "│ a   ┆ achievements ┆ across ┆ all ┆ … ┆ to  ┆ today ┆ trust ┆ you │\n",
       "│ --- ┆ ---          ┆ ---    ┆ --- ┆   ┆ --- ┆ ---   ┆ ---   ┆ --- │\n",
       "│ i32 ┆ i32          ┆ i32    ┆ i32 ┆   ┆ i32 ┆ i32   ┆ i32   ┆ i32 │\n",
       "╞═════╪══════════════╪════════╪═════╪═══╪═════╪═══════╪═══════╪═════╡\n",
       "│ 2   ┆ 0            ┆ 0      ┆ 0   ┆ … ┆ 0   ┆ 1     ┆ 0     ┆ 1   │\n",
       "│ 1   ┆ 1            ┆ 1      ┆ 1   ┆ … ┆ 1   ┆ 0     ┆ 1     ┆ 0   │\n",
       "│ 0   ┆ 0            ┆ 0      ┆ 0   ┆ … ┆ 0   ┆ 0     ┆ 0     ┆ 0   │\n",
       "└─────┴──────────────┴────────┴─────┴───┴─────┴───────┴───────┴─────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df: pl.DataFrame = pl.DataFrame(freq_count)\n",
    "freq_df.columns = [*vocab.keys()]\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 56)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.111111</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.0</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.055556</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055556</td><td>0.055556</td><td>0.0</td><td>0.055556</td><td>0.0</td><td>0.055556</td></tr><tr><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.081081</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.027027</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.027027</td><td>0.027027</td><td>0.0</td><td>0.027027</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.083333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.125</td><td>0.083333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.083333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.041667</td><td>0.041667</td><td>0.041667</td><td>0.0</td><td>0.0</td><td>0.083333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.083333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.333333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 56)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ to       ┆ today    ┆ trust    ┆ you      │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0.111111 ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.055556 ┆ 0.0      ┆ 0.055556 │\n",
       "│ 0.027027 ┆ 0.027027     ┆ 0.027027 ┆ 0.027027 ┆ … ┆ 0.027027 ┆ 0.0      ┆ 0.027027 ┆ 0.0      │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0      ┆ 0.0      │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_df: pl.DataFrame = pl.DataFrame(term_freq)\n",
    "term_freq_df.columns = [*vocab.keys()]\n",
    "term_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "- The inverse document frequency is a `measure of how much information the word provides`, i.e., how common or rare it is across all documents.\n",
    "- It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient)\n",
    "\n",
    "$$ n_{t} = |d \\in D: t \\in d| $$\n",
    "$$ idf_{(t,D)} = log (\\frac{N}{n_{t} + 1}) + 1 $$\n",
    "\n",
    "- where $N$ is the total number of documents in the corpus, $t \\in d$ is the number of terms in a document, $d \\in D$ is a document in the corpus $D$ and 1 is added to the denominator to avoid division-by-zero errors.\n",
    "- If the term is not in the corpus, this will lead to a division-by-zero.\n",
    "  - It is therefore common to adjust the numerator and denominator by adding a smoothing term to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84729786, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        0.84729786, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        0.84729786, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 0.84729786,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229, 0.69314718, 1.09861229, 1.09861229, 1.09861229,\n",
       "        1.09861229]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(term_freq != 0)[:, 7].sum()\n",
    "\n",
    "N: int = len(corpus)  # number of documents\n",
    "S_F: int = 1  # smoothing factor\n",
    "\n",
    "# Number of documents containing a term\n",
    "doc_freq: np.ndarray = term_freq != 0\n",
    "doc_freq = doc_freq.sum(axis=0, keepdims=True)\n",
    "\n",
    "idf: np.ndarray = np.log1p((N + S_F) / (doc_freq + S_F))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 56)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.847298</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>0.847298</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>0.847298</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>0.847298</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>0.693147</td><td>1.098612</td><td>1.098612</td><td>1.098612</td><td>1.098612</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 56)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ to       ┆ today    ┆ trust    ┆ you      │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0.847298 ┆ 1.098612     ┆ 1.098612 ┆ 1.098612 ┆ … ┆ 1.098612 ┆ 1.098612 ┆ 1.098612 ┆ 1.098612 │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_df: pl.DataFrame = pl.DataFrame(idf)\n",
    "idf_df.columns = [*vocab.keys()]\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 56)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.092277</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046138</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046138</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.037744</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.059823</td></tr><tr><td>0.021874</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.065622</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.021874</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.021874</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.017894</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.074915</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.145702</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.048567</td><td>0.048567</td><td>0.048567</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.245141</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 56)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ to       ┆ today    ┆ trust    ┆ you      │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0.092277 ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.059823 ┆ 0.0      ┆ 0.059823 │\n",
       "│ 0.021874 ┆ 0.028362     ┆ 0.028362 ┆ 0.028362 ┆ … ┆ 0.028362 ┆ 0.0      ┆ 0.028362 ┆ 0.0      │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0      ┆ 0.0      │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf: np.ndarray = term_freq * idf\n",
    "# Normalize\n",
    "tf_idf = tf_idf / tf_idf.sum(axis=-1, keepdims=True)\n",
    "\n",
    "tf_idf_df: pl.DataFrame = pl.DataFrame(tf_idf)\n",
    "tf_idf_df.columns = [*vocab.keys()]\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>total</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.092277</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046138</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046138</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.037744</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.059823</td><td>1.0</td></tr><tr><td>0.021874</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.065622</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.021874</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.021874</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.017894</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.0</td><td>1.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.074915</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.145702</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.048567</td><td>0.048567</td><td>0.048567</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.245141</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 57)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬───────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ today    ┆ trust    ┆ you      ┆ total │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---   │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64   │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪═══════╡\n",
       "│ 0.092277 ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.059823 ┆ 0.0      ┆ 0.059823 ┆ 1.0   │\n",
       "│ 0.021874 ┆ 0.028362     ┆ 0.028362 ┆ 0.028362 ┆ … ┆ 0.0      ┆ 0.028362 ┆ 0.0      ┆ 1.0   │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0      ┆ 1.0   │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴───────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df.with_columns(total=pl.sum_horizontal(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world from python'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lower(text: str | list[str]) -> str | list[str]:\n",
    "    \"\"\"\n",
    "    Convert input text to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text (str | list[str]): Input text or list of strings to convert.\n",
    "\n",
    "    Returns:\n",
    "        str | list[str]: Lowercase version of input text or list of lowercase strings.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input is not a string or list of strings.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    elif isinstance(text, list):\n",
    "        return [t.lower() if isinstance(t, str) else t for t in text]\n",
    "    else:\n",
    "        raise TypeError(f\"Expected str or list of str, got {type(text).__name__}\")\n",
    "\n",
    "\n",
    "to_lower(\"Hello World from Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    \"\"\"\n",
    "    A class for calculating TF-IDF (Term Frequency-Inverse Document Frequency) scores\n",
    "    for a corpus of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_corpus: list[list[str]], vocab: dict[str, int]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TFIDF class with a corpus and vocabulary.\n",
    "\n",
    "        Args:\n",
    "            corpus (list[list[str]]): A list of documents, where each document is a list of tokenized words.\n",
    "            vocab (dict[str, int]): A dictionary mapping words to their indices in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.corpus: list[list[str]] = tok_corpus\n",
    "        self.vocab: dict[str, int] = vocab\n",
    "\n",
    "    def _calculate_tf(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the term frequency (TF) for each term in each document.\n",
    "        \"\"\"\n",
    "        # (n_docs, n_terms)\n",
    "        freq_count: np.ndarray = np.zeros(\n",
    "            (len(self.corpus), len(self.vocab)), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        for idx, doc in enumerate(self.corpus, start=0):\n",
    "            for word in doc:\n",
    "                word_idx: int = self.vocab[word]\n",
    "                freq_count[idx, word_idx] += 1\n",
    "\n",
    "        tf: np.ndarray = freq_count / np.sum(freq_count, axis=1, keepdims=True)\n",
    "        return tf\n",
    "\n",
    "    def _calculate_idf(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the inverse document frequency (IDF) for each term.\n",
    "        \"\"\"\n",
    "        N: int = len(self.corpus)  # number of documents\n",
    "        S_F: int = 1  # smoothing factor\n",
    "        tf: np.ndarray = self._calculate_tf()\n",
    "\n",
    "        # Number of documents containing a term\n",
    "        doc_freq: np.ndarray = tf > 0\n",
    "        doc_freq: np.ndarray = doc_freq.sum(axis=0, keepdims=True)\n",
    "        idf: np.ndarray = np.log1p((N + S_F) / (doc_freq + S_F))\n",
    "\n",
    "        return idf\n",
    "\n",
    "    def calculate_tfidf(self, normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the TF-IDF scores for the corpus.\n",
    "        \"\"\"\n",
    "        tf: np.ndarray = self._calculate_tf()\n",
    "        idf: np.ndarray = self._calculate_idf()\n",
    "        tf_idf: np.ndarray = tf * idf\n",
    "        if normalize:\n",
    "            tf_idf = tf_idf / tf_idf.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 56)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.092277</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046138</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.059823</td><td>0.059823</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046138</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.059823</td><td>0.037744</td><td>0.0</td><td>0.059823</td><td>0.0</td><td>0.059823</td></tr><tr><td>0.021874</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.065622</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.021874</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028362</td><td>0.021874</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.028362</td><td>0.0</td><td>0.017894</td><td>0.028362</td><td>0.0</td><td>0.028362</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.074915</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.145702</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.048567</td><td>0.048567</td><td>0.048567</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.097135</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.245141</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 56)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ to       ┆ today    ┆ trust    ┆ you      │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0.092277 ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.059823 ┆ 0.0      ┆ 0.059823 │\n",
       "│ 0.021874 ┆ 0.028362     ┆ 0.028362 ┆ 0.028362 ┆ … ┆ 0.028362 ┆ 0.0      ┆ 0.028362 ┆ 0.0      │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0      ┆ 0.0      │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer: TFIDF = TFIDF(tok_corpus=corpus, vocab=vocab)\n",
    "tf_idf: np.ndarray = vectorizer.calculate_tfidf(normalize=True)\n",
    "\n",
    "tf_idf_df_2: pl.DataFrame = pl.DataFrame(tf_idf)\n",
    "tf_idf_df_2.columns = [*vocab.keys()]\n",
    "tf_idf_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying `Dot Product` And `Term Count` Vectors To Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(content: str, term: str) -> float:\n",
    "    \"\"\"\n",
    "    Count the occurrences of a term in the given content.\n",
    "\n",
    "    Args:\n",
    "        content (str): The text content to search in.\n",
    "        term (str): The term to search for.\n",
    "\n",
    "    Returns:\n",
    "        float: The number of occurrences of the term in the content.\n",
    "    \"\"\"\n",
    "    tokenized_content: list[str] = tokenize(content)\n",
    "    term_count: int = tokenized_content.count(term.lower())\n",
    "\n",
    "    return float(term_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_query = ['hi', 'world']\n",
      "query_vector = [1, 1]\n",
      "count_hello = 1.0\n"
     ]
    }
   ],
   "source": [
    "text: str = \"Hello world, hi everyone in the world\"\n",
    "query: str = \"hi world\"\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "print(f\"{tok_query = }\")\n",
    "query_vector: list[int] = [1 for _ in tok_query]\n",
    "print(f\"{query_vector = }\")\n",
    "count_hello: int = term_count(content=text, term=\"hello\")\n",
    "print(f\"{count_hello = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi: 1.0', 'world: 2.0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Hello world, hi everyone in the world\"\n",
    "[f\"{t}: {term_count(content=text, term=t)}\" for t in tok_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_query = ['the', 'cat', 'in', 'the', 'hat']\n",
      "query_vector = [1, 1, 1, 1, 1]\n",
      "doc_vectors = [[5.0, 0.0, 4.0, 5.0, 0.0], [3.0, 2.0, 2.0, 3.0, 2.0], [0.0, 1.0, 2.0, 0.0, 1.0]]\n",
      "\n",
      "doc_scores = [14.0, 12.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "doc1: str = \"\"\"In light of the big reveal in her interview, the interesting\n",
    "          thing is that the person in the wrong probably made a good\n",
    "          decision in the end.\"\"\"\n",
    "doc2: str = \"\"\"My favorite book is the cat in the hat, which is about a crazy\n",
    "          cat in a hat who breaks into a house and creates the craziest\n",
    "          afternoon for two kids.\"\"\"\n",
    "doc3: str = \"\"\"My careless neighbors apparently let a stray cat stay in their\n",
    "          garage unsupervised, which resulted in my favorite hat that I\n",
    "          let them borrow being ruined.\"\"\"\n",
    "\n",
    "query: str = \"the cat in the hat\"\n",
    "\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "# Count vectors\n",
    "query_vector: list[int] = [1 for t in tok_query]\n",
    "print(f\"{tok_query = }\")\n",
    "print(f\"{query_vector = }\")\n",
    "\n",
    "# Count vectors\n",
    "doc_vectors: list[list[float]] = [\n",
    "    [term_count(content=doc, term=tok) for tok in tok_query]\n",
    "    for doc in [doc1, doc2, doc3]\n",
    "]\n",
    "print(f\"{doc_vectors = }\")\n",
    "\n",
    "doc_scores: list[float] = [\n",
    "    np.dot(query_vector, doc_vector) for doc_vector in doc_vectors\n",
    "]\n",
    "print()\n",
    "print(f\"{doc_scores = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "\n",
    "- The current ranking system prioritizes documents with the most keywords, not necessarily the most relevant ones.\n",
    "- This means documents with common words (like \"the\" and \"in\") rank higher than those with all the specific keywords, even if they appear less frequently.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Replace `Term Count` With `Term Frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(tok_corpus: list[list[str]], vocab: dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the term frequency (TF) for each term in each document.\n",
    "    \"\"\"\n",
    "    # (n_docs, n_terms)\n",
    "    freq_count: np.ndarray = np.zeros((len(tok_corpus), len(vocab)), dtype=np.int32)\n",
    "\n",
    "    for idx, doc in enumerate(tok_corpus, start=0):\n",
    "        for word in doc:\n",
    "            word_idx: int = vocab[word]\n",
    "            freq_count[idx, word_idx] += 1\n",
    "\n",
    "    tf: np.ndarray = freq_count / np.sum(freq_count, axis=1, keepdims=True)\n",
    "    return tf\n",
    "\n",
    "\n",
    "def calculate_idf(tok_corpus: list[list[str]], tf: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) for each term.\n",
    "    \"\"\"\n",
    "    N: int = len(tok_corpus)  # number of documents\n",
    "    S_F: int = 1  # smoothing factor\n",
    "\n",
    "    # Number of documents containing a term\n",
    "    doc_freq: np.ndarray = tf > 0\n",
    "    doc_freq: np.ndarray = doc_freq.sum(axis=0, keepdims=True)\n",
    "    idf: np.ndarray = np.log1p((N + S_F) / (doc_freq + S_F))\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def calculate_tfidf(\n",
    "    tf: np.ndarray, idf: np.ndarray, normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the TF-IDF scores for the corpus.\n",
    "    \"\"\"\n",
    "    tf_idf: np.ndarray = tf * idf\n",
    "    if normalize:\n",
    "        tf_idf = tf_idf / tf_idf.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.25, 0.0, 0.25, 0.0, 0.0, 0.25, 0.25],\n",
       " [0.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_corpus: list[list[str]] = (\n",
    "    [\"my\", \"president\", \"is\", \"black\"],\n",
    "    [\"i\", \"love\", \"jesus\"],\n",
    ")\n",
    "vocab: dict[str, int] = {\n",
    "    \"black\": 0,\n",
    "    \"i\": 1,\n",
    "    \"is\": 2,\n",
    "    \"jesus\": 3,\n",
    "    \"love\": 4,\n",
    "    \"my\": 5,\n",
    "    \"president\": 6,\n",
    "}\n",
    "\n",
    "tf: np.ndarray = calculate_tf(tok_corpus=tok_corpus, vocab=vocab)\n",
    "tf.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query: str = \"white pressident\"\n",
    "\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "# Count vectors\n",
    "query_vector: list[int] = [1 for t in tok_query]\n",
    "print(f\"{tok_query = }\")\n",
    "print(f\"{query_vector = }\")\n",
    "\n",
    "# Term freq vectors\n",
    "# TODO: Fix function so that it uses the term freq instead of the count\n",
    "doc_vectors: list[list[float]] = [\n",
    "    [calculate_tf(content=doc, term=tok) for tok in tok_query]\n",
    "    for doc in [doc1, doc2, doc3]\n",
    "]\n",
    "print(f\"{doc_vectors = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_query = ['the', 'cat', 'in', 'the', 'hat']\n",
      "query_vector = [1, 1, 1, 1, 1]\n",
      "doc_vectors = [[5.0, 0.0, 4.0, 5.0, 0.0], [3.0, 2.0, 2.0, 3.0, 2.0], [0.0, 1.0, 2.0, 0.0, 1.0]]\n",
      "\n",
      "doc_scores = [14.0, 12.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "query: str = \"the cat in the hat\"\n",
    "\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "# Count vectors\n",
    "query_vector: list[int] = [1 for t in tok_query]\n",
    "print(f\"{tok_query = }\")\n",
    "print(f\"{query_vector = }\")\n",
    "\n",
    "# Count vectors\n",
    "doc_vectors: list[list[float]] = [\n",
    "    [term_count(content=doc, term=tok) for tok in tok_query]\n",
    "    for doc in [doc1, doc2, doc3]\n",
    "]\n",
    "print(f\"{doc_vectors = }\")\n",
    "\n",
    "doc_scores: list[float] = [\n",
    "    np.dot(query_vector, doc_vector) for doc_vector in doc_vectors\n",
    "]\n",
    "print()\n",
    "print(f\"{doc_scores = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'black': 0, 'i': 1, 'is': 2, 'jesus': 3, 'love': 4, 'my': 5, 'president': 6}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_vocab(doc=[[\"my president is black\"], [\"i love jesus\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.25, 0.0, 0.25, 0.0, 0.0, 0.25, 0.25],\n",
       " [0.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_corpus: list[list[str]] = (\n",
    "    [\"my\", \"president\", \"is\", \"black\"],\n",
    "    [\"i\", \"love\", \"jesus\"],\n",
    ")\n",
    "vocab: dict[str, int] = {\n",
    "    \"black\": 0,\n",
    "    \"i\": 1,\n",
    "    \"is\": 2,\n",
    "    \"jesus\": 3,\n",
    "    \"love\": 4,\n",
    "    \"my\": 5,\n",
    "    \"president\": 6,\n",
    "}\n",
    "\n",
    "tf: np.ndarray = calculate_tf(tok_corpus=tok_corpus, vocab=vocab)\n",
    "tf.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying TF-IDF To Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([\"Hello world, hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern: str = r'([,.?_!\"()\\':]|\\s)'\n",
    "text: str = \"Oya hail my Jesus ...\"\n",
    "\n",
    "tokenize(doc=[text], drop_punct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311 (ai_search)",
   "language": "python",
   "name": "ai_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
