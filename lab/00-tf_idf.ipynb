{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Search\n",
    "\n",
    "Using:\n",
    "- **Count**\n",
    "- **Term Frequency**\n",
    "- **Dampened Term Frequency**\n",
    "- **Term Frequency Inverse Document Frequency**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.9\n",
      "IPython version      : 8.26.0\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.2\n",
      "polars   : 1.0.0\n",
      "torch    : 2.2.2\n",
      "lightning: 2.3.2\n",
      "\n",
      "conda environment: ai_search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'happy': 1, 'i': 2, 'today': 3, 'very': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_token: str = \"ukn\"\n",
    "\n",
    "text: list[str] = [\"i\", \"am\", \"very\", \"happy\", \"today\"]\n",
    "vocab_set: set = sorted(set(text))\n",
    "vocab: dict[str, int] = {word: idx for idx, word in enumerate(vocab_set, start=0)}\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'very', 'happy', 'today']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern: str = r'([,.?_!\"()\\']|--|\\s)'\n",
    "result: list[str] = re.split(pattern=pattern, string=\" \".join(text))\n",
    "result = [x for x in result if x.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def tokenize(doc: str | list[str], drop_punct: bool = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the input document and optionally remove punctuation.\n",
    "\n",
    "    Args:\n",
    "        doc (str | list[str]): The input document as a string or list of strings.\n",
    "        drop_punct (bool, optional): Whether to remove punctuation. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The tokenized document as a list of strings.\n",
    "    \"\"\"\n",
    "    # Pattern for separating tokens\n",
    "    pattern: str = r'([,.?_!\"()\\':]|\\s)'\n",
    "\n",
    "    if isinstance(doc, str):\n",
    "        tok_doc: list[str] = re.split(pattern=pattern, string=doc.lower())\n",
    "\n",
    "    if isinstance(doc, list):\n",
    "        doc = [word.lower() for word in doc]\n",
    "        tok_doc = re.split(pattern=pattern, string=\" \".join(doc))\n",
    "\n",
    "    # Remove whitespaces and empty strings\n",
    "    tok_doc = [word for word in tok_doc if word.strip()]\n",
    "\n",
    "    if drop_punct:\n",
    "        tok_doc = [word for word in tok_doc if word not in string.punctuation]\n",
    "\n",
    "    return tok_doc\n",
    "\n",
    "\n",
    "def flatten_documents(docs: list[list[str]]) -> list[str]:\n",
    "\n",
    "    assert all(\n",
    "        [True if isinstance(row, list) else False for row in docs]\n",
    "    ), \"Not all elements are lists\"\n",
    "\n",
    "    flattened_doc: list[str] = [word.lower() for row in docs for word in row]\n",
    "    return flattened_doc\n",
    "\n",
    "\n",
    "def generate_vocab(docs: list[list[str]], drop_punct: bool = True) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Generate a vocabulary dictionary from the given document.\n",
    "\n",
    "    Args:\n",
    "        docs (list[list[str]]): The input document as a list of lists of strings.\n",
    "        drop_punct (bool, optional): Whether to drop punctuation during tokenization. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, int]: A dictionary mapping words to their indices in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    unk_tok: str = \"ukn\"\n",
    "    flattened_doc: list[str] = flatten_documents(docs)\n",
    "    tok_doc: list[str] = tokenize(flattened_doc, drop_punct=drop_punct)\n",
    "    tok_doc = sorted(set(tok_doc))\n",
    "\n",
    "    vocab: dict[str, int] = {word: idx for idx, word in enumerate(tok_doc, start=0)}\n",
    "    # Add unknown token\n",
    "    vocab[unk_tok] = len(vocab)\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def encode(doc: list[list[str]], vocab: dict[str, int]) -> list[int]:\n",
    "    \"\"\"This particular implementation checks for the occurrence of a term.\"\"\"\n",
    "    unk_tok: str = \"ukn\"\n",
    "    arr: np.ndarray = np.zeros((1, len(vocab)), dtype=int)\n",
    "    for row in doc:\n",
    "        for word in tokenize(row):\n",
    "            if word in vocab:\n",
    "                arr[0, vocab.get(word, unk_tok)] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def encode_n_create_df(doc: list[list[str]], vocab: dict[str, int]) -> pl.DataFrame:\n",
    "    df: pl.DataFrame = pl.DataFrame(encode(doc=doc, vocab=vocab))\n",
    "    df.columns = list(vocab.keys())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7\n",
      "vocab = {'about': 0, 'hey': 1, 'me': 2, 'neidu': 3, 'something': 4, 'tell': 5, 'ukn': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs: list[list[str]] = [[\"Hey! Tell me something about neidu.\"]]\n",
    "vocab: dict[str, int] = generate_vocab(docs=docs, drop_punct=True)\n",
    "print(f\"{vocab = }\")\n",
    "\n",
    "encode(doc=[[\"tell Tell\"]], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>about</th><th>hey</th><th>me</th><th>neidu</th><th>something</th><th>tell</th><th>ukn</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 7)\n",
       "┌───────┬─────┬─────┬───────┬───────────┬──────┬─────┐\n",
       "│ about ┆ hey ┆ me  ┆ neidu ┆ something ┆ tell ┆ ukn │\n",
       "│ ---   ┆ --- ┆ --- ┆ ---   ┆ ---       ┆ ---  ┆ --- │\n",
       "│ i64   ┆ i64 ┆ i64 ┆ i64   ┆ i64       ┆ i64  ┆ i64 │\n",
       "╞═══════╪═════╪═════╪═══════╪═══════════╪══════╪═════╡\n",
       "│ 1     ┆ 0   ┆ 0   ┆ 0     ┆ 0         ┆ 1    ┆ 0   │\n",
       "└───────┴─────┴─────┴───────┴───────────┴──────┴─────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_n_create_df(doc=[[\"about Tell\"]], vocab=vocab)\n",
    "\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "#### [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity#:~:text=Cosine%20similarity%20is%20the%20cosine,but%20only%20on%20their%20angle.)\n",
    "\n",
    "- Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths.\n",
    "- It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle.\n",
    "\n",
    "$$cosSimilarity = cos(\\theta) = \\frac{A . B}{||A||.||B||}$$\n",
    "\n",
    "- $||A||$ is the length of vector A, and $||B||$ is the length of vector B. aka. norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray) -> float:\n",
    "    return np.dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
    "\n",
    "\n",
    "def check_equality(vector_1: np.ndarray, vector_2: np.ndarray) -> bool:\n",
    "    result: bool = np.array_equal(vector_1, vector_2)\n",
    "    print(f\"{result = }\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746318461970762"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector1=np.array([1, 2, 3]), vector2=np.array([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 49\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "doc1: list[str] = [\n",
    "    (\n",
    "        \"Lynn: ham and cheese sandwich, chocolate cookie, ice water. \"\n",
    "        \"Brian: turkey avocado sandwich, plain potato chips, apple juice \"\n",
    "        \"Mohammed: grilled chicken salad, fruit cup, lemonade \"\n",
    "    )\n",
    "]\n",
    "\n",
    "doc2: list[str] = [\n",
    "    (\n",
    "        \"Orchard Farms apple juice is premium, organic apple juice made from the \"\n",
    "        \"freshest apples, never from concentrate. Its juice has received the \"\n",
    "        \"regional award for best apple juice three years in a row. \"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "docs: list[list[str]] = [doc1, doc2]\n",
    "vocab: dict[str, int] = generate_vocab(docs=docs)\n",
    "\n",
    "query: list[list[str]] = [[\"apple juice\"]]\n",
    "query_vector: np.ndarray = encode(doc=query, vocab=vocab)\n",
    "doc1_vector: np.ndarray = encode(doc=[doc1], vocab=vocab)\n",
    "doc2_vector: np.ndarray = encode(doc=[doc2], vocab=vocab)\n",
    "\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'apple'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'apples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'avocado'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'award'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'best'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'brian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cheese'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chicken'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chips'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chocolate'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'concentrate'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cookie'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cup'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'farms'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'for'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'freshest'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'from'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'fruit'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'grilled'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ham'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'has'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ice'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'its'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'juice'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lemonade'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lynn'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'made'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'mohammed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'never'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'orchard'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'organic'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'plain'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'potato'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'premium'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'received'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'regional'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'row'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'salad'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sandwich'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'three'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'turkey'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'water'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'years'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ukn'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'a'\u001b[0m,\n",
       "    \u001b[32m'and'\u001b[0m,\n",
       "    \u001b[32m'apple'\u001b[0m,\n",
       "    \u001b[32m'apples'\u001b[0m,\n",
       "    \u001b[32m'avocado'\u001b[0m,\n",
       "    \u001b[32m'award'\u001b[0m,\n",
       "    \u001b[32m'best'\u001b[0m,\n",
       "    \u001b[32m'brian'\u001b[0m,\n",
       "    \u001b[32m'cheese'\u001b[0m,\n",
       "    \u001b[32m'chicken'\u001b[0m,\n",
       "    \u001b[32m'chips'\u001b[0m,\n",
       "    \u001b[32m'chocolate'\u001b[0m,\n",
       "    \u001b[32m'concentrate'\u001b[0m,\n",
       "    \u001b[32m'cookie'\u001b[0m,\n",
       "    \u001b[32m'cup'\u001b[0m,\n",
       "    \u001b[32m'farms'\u001b[0m,\n",
       "    \u001b[32m'for'\u001b[0m,\n",
       "    \u001b[32m'freshest'\u001b[0m,\n",
       "    \u001b[32m'from'\u001b[0m,\n",
       "    \u001b[32m'fruit'\u001b[0m,\n",
       "    \u001b[32m'grilled'\u001b[0m,\n",
       "    \u001b[32m'ham'\u001b[0m,\n",
       "    \u001b[32m'has'\u001b[0m,\n",
       "    \u001b[32m'ice'\u001b[0m,\n",
       "    \u001b[32m'in'\u001b[0m,\n",
       "    \u001b[32m'is'\u001b[0m,\n",
       "    \u001b[32m'its'\u001b[0m,\n",
       "    \u001b[32m'juice'\u001b[0m,\n",
       "    \u001b[32m'lemonade'\u001b[0m,\n",
       "    \u001b[32m'lynn'\u001b[0m,\n",
       "    \u001b[32m'made'\u001b[0m,\n",
       "    \u001b[32m'mohammed'\u001b[0m,\n",
       "    \u001b[32m'never'\u001b[0m,\n",
       "    \u001b[32m'orchard'\u001b[0m,\n",
       "    \u001b[32m'organic'\u001b[0m,\n",
       "    \u001b[32m'plain'\u001b[0m,\n",
       "    \u001b[32m'potato'\u001b[0m,\n",
       "    \u001b[32m'premium'\u001b[0m,\n",
       "    \u001b[32m'received'\u001b[0m,\n",
       "    \u001b[32m'regional'\u001b[0m,\n",
       "    \u001b[32m'row'\u001b[0m,\n",
       "    \u001b[32m'salad'\u001b[0m,\n",
       "    \u001b[32m'sandwich'\u001b[0m,\n",
       "    \u001b[32m'the'\u001b[0m,\n",
       "    \u001b[32m'three'\u001b[0m,\n",
       "    \u001b[32m'turkey'\u001b[0m,\n",
       "    \u001b[32m'water'\u001b[0m,\n",
       "    \u001b[32m'years'\u001b[0m,\n",
       "    \u001b[32m'ukn'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2886751345948129, 0.2773500981126146)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_score = cosine_similarity(query_vector.squeeze(), doc1_vector.squeeze())\n",
    "doc2_score = cosine_similarity(query_vector.squeeze(), doc2_vector.squeeze())\n",
    "\n",
    "doc1_score, doc2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 49)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>and</th><th>apple</th><th>apples</th><th>avocado</th><th>award</th><th>best</th><th>brian</th><th>cheese</th><th>chicken</th><th>chips</th><th>chocolate</th><th>concentrate</th><th>cookie</th><th>cup</th><th>farms</th><th>for</th><th>freshest</th><th>from</th><th>fruit</th><th>grilled</th><th>ham</th><th>has</th><th>ice</th><th>in</th><th>is</th><th>its</th><th>juice</th><th>lemonade</th><th>lynn</th><th>made</th><th>mohammed</th><th>never</th><th>orchard</th><th>organic</th><th>plain</th><th>potato</th><th>premium</th><th>received</th><th>regional</th><th>row</th><th>salad</th><th>sandwich</th><th>the</th><th>three</th><th>turkey</th><th>water</th><th>years</th><th>ukn</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 49)\n",
       "┌─────┬─────┬───────┬────────┬───┬────────┬───────┬───────┬─────┐\n",
       "│ a   ┆ and ┆ apple ┆ apples ┆ … ┆ turkey ┆ water ┆ years ┆ ukn │\n",
       "│ --- ┆ --- ┆ ---   ┆ ---    ┆   ┆ ---    ┆ ---   ┆ ---   ┆ --- │\n",
       "│ i64 ┆ i64 ┆ i64   ┆ i64    ┆   ┆ i64    ┆ i64   ┆ i64   ┆ i64 │\n",
       "╞═════╪═════╪═══════╪════════╪═══╪════════╪═══════╪═══════╪═════╡\n",
       "│ 0   ┆ 1   ┆ 1     ┆ 0      ┆ … ┆ 1      ┆ 1     ┆ 0     ┆ 0   │\n",
       "└─────┴─────┴───────┴────────┴───┴────────┴───────┴───────┴─────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_n_create_df(doc=[doc1], vocab=vocab)\n",
    "# encode(doc=[doc1], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 49)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>and</th><th>apple</th><th>apples</th><th>avocado</th><th>award</th><th>best</th><th>brian</th><th>cheese</th><th>chicken</th><th>chips</th><th>chocolate</th><th>concentrate</th><th>cookie</th><th>cup</th><th>farms</th><th>for</th><th>freshest</th><th>from</th><th>fruit</th><th>grilled</th><th>ham</th><th>has</th><th>ice</th><th>in</th><th>is</th><th>its</th><th>juice</th><th>lemonade</th><th>lynn</th><th>made</th><th>mohammed</th><th>never</th><th>orchard</th><th>organic</th><th>plain</th><th>potato</th><th>premium</th><th>received</th><th>regional</th><th>row</th><th>salad</th><th>sandwich</th><th>the</th><th>three</th><th>turkey</th><th>water</th><th>years</th><th>ukn</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 49)\n",
       "┌─────┬─────┬───────┬────────┬───┬────────┬───────┬───────┬─────┐\n",
       "│ a   ┆ and ┆ apple ┆ apples ┆ … ┆ turkey ┆ water ┆ years ┆ ukn │\n",
       "│ --- ┆ --- ┆ ---   ┆ ---    ┆   ┆ ---    ┆ ---   ┆ ---   ┆ --- │\n",
       "│ i64 ┆ i64 ┆ i64   ┆ i64    ┆   ┆ i64    ┆ i64   ┆ i64   ┆ i64 │\n",
       "╞═════╪═════╪═══════╪════════╪═══╪════════╪═══════╪═══════╪═════╡\n",
       "│ 1   ┆ 0   ┆ 1     ┆ 1      ┆ … ┆ 0      ┆ 0     ┆ 1     ┆ 0   │\n",
       "└─────┴─────┴───────┴────────┴───┴────────┴───────┴───────┴─────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_n_create_df(doc=[doc2], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = True\n",
      "result = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality(doc1_vector.squeeze(), doc1_vector.squeeze())\n",
    "check_equality(doc1_vector.squeeze(), doc2_vector.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### [Term Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "- Term frequency, tf(t,d), is the relative frequency of term t within document d.\n",
    "- where $f_{t,d}$ is the number of times term t appears in document d and $\\sum_{t' \\in d} f_{t',d}$ is the total number of terms in document d.\n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}$$\n",
    "\n",
    "#### Dampen the Term Frequency\n",
    "\n",
    "- Reduce weight of common words within documents.\n",
    "- i.e. Lower importance of frequent terms per document.\n",
    "\n",
    "$$log_{tf} = 1 + log(freq_{count} + 1)$$\n",
    "\n",
    "- Here, 1 is added to both the logarithm and the frequency count to avoid taking the log of zero and to dampen the effect of term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 57\n",
      "vocab = {'a': 0, 'achievements': 1, 'across': 2, 'all': 3, 'am': 4, 'and': 5, 'beautiful': 6, 'bedrock': 7, 'builds': 8, 'cat': 9, 'chased': 10, 'consistency': 11, 'creating': 12, 'day': 13, 'definitely': 14, 'dog': 15, 'drives': 16, 'enabling': 17, 'endeavors': 18, 'excellence': 19, 'for': 20, 'fosters': 21, 'foundation': 22, 'from': 23, 'gift': 24, 'grateful': 25, 'growth': 26, 'habits': 27, 'i': 28, 'improvement': 29, 'in': 30, 'is': 31, 'it': 32, 'jesus': 33, 'life': 34, 'log': 35, 'mat': 36, 'mouse': 37, 'myriad': 38, 'of': 39, 'on': 40, 'personal': 41, 'professional': 42, 'progress': 43, 'pursuits': 44, 'reliability': 45, 's': 46, 'sat': 47, 'steady': 48, 'success': 49, 'thank': 50, 'the': 51, 'to': 52, 'today': 53, 'trust': 54, 'you': 55, 'ukn': 56}\n",
      "\n",
      "corpus = [['thank', 'you', 'jesus', 'for', 'the', 'gift', 'of', 'today', 'i', 'am', 'grateful', 'it', 'is', 'a', 'definitely', 'a', 'beautiful', 'day'], ['consistency', 'the', 'bedrock', 'of', 'progress', 'and', 'reliability', 'fosters', 'trust', 'builds', 'habits', 'and', 'drives', 'success', 'across', 'all', 'endeavors', 'from', 'personal', 'growth', 'to', 'professional', 'achievements', 'enabling', 'steady', 'improvement', 'and', 'creating', 'a', 'foundation', 'for', 'excellence', 'in', 'life', 's', 'myriad', 'pursuits'], ['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on', 'the', 'log', 'and', 'the', 'cat', 'chased', 'the', 'mouse', 'and', 'the', 'dog', 'chased', 'the', 'cat']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc1: list[str] = [\n",
    "    (\n",
    "        \"Thank you Jesus for the gift of today. I am grateful. \"\n",
    "        \"It is a definitely a beautiful day.\"\n",
    "    )\n",
    "]\n",
    "doc2: list[str] = [\n",
    "    (\n",
    "        \"Consistency, the bedrock of progress and reliability, fosters trust, \"\n",
    "        \"builds habits, and drives success across all endeavors, from personal \"\n",
    "        \"growth to professional achievements, enabling steady improvement and \"\n",
    "        \"creating a foundation for excellence in life's myriad pursuits.\"\n",
    "    )\n",
    "]\n",
    "doc3: list[str] = [\n",
    "    (\n",
    "        \"the cat sat on the mat the dog sat on the log \"\n",
    "        \"and the cat chased the mouse and the dog chased the cat\"\n",
    "    )\n",
    "]\n",
    "corpus: list[list[str]] = [tokenize(doc1), tokenize(doc2), tokenize(doc3)]\n",
    "vocab: dict[str, int] = generate_vocab(docs=corpus)\n",
    "print(f\"{vocab = }\\n\")\n",
    "print(f\"{corpus = }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.        , 0.        , 0.        , 0.05555556,\n",
       "        0.        , 0.05555556, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05555556, 0.05555556,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05555556, 0.        , 0.        , 0.        , 0.05555556,\n",
       "        0.05555556, 0.        , 0.        , 0.05555556, 0.        ,\n",
       "        0.        , 0.05555556, 0.05555556, 0.05555556, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05555556,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05555556, 0.05555556, 0.        , 0.05555556, 0.        ,\n",
       "        0.05555556, 0.        ],\n",
       "       [0.02702703, 0.02702703, 0.02702703, 0.02702703, 0.        ,\n",
       "        0.08108108, 0.        , 0.02702703, 0.02702703, 0.        ,\n",
       "        0.        , 0.02702703, 0.02702703, 0.        , 0.        ,\n",
       "        0.        , 0.02702703, 0.02702703, 0.02702703, 0.02702703,\n",
       "        0.02702703, 0.02702703, 0.02702703, 0.02702703, 0.        ,\n",
       "        0.        , 0.02702703, 0.02702703, 0.        , 0.02702703,\n",
       "        0.02702703, 0.        , 0.        , 0.        , 0.02702703,\n",
       "        0.        , 0.        , 0.        , 0.02702703, 0.02702703,\n",
       "        0.        , 0.02702703, 0.02702703, 0.02702703, 0.02702703,\n",
       "        0.02702703, 0.02702703, 0.        , 0.02702703, 0.02702703,\n",
       "        0.        , 0.02702703, 0.02702703, 0.        , 0.02702703,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.125     ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08333333, 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (n_docs, n_terms)\n",
    "freq_count: np.ndarray = np.zeros((len(corpus), len(vocab)), dtype=np.int32)\n",
    "\n",
    "for idx, doc in enumerate(corpus, start=0):\n",
    "    for word in doc:\n",
    "        word_idx = vocab[word]\n",
    "        freq_count[idx, word_idx] += 1\n",
    "\n",
    "\n",
    "term_freq: np.ndarray = freq_count / np.sum(freq_count, axis=1, keepdims=True)\n",
    "term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10857028, 0.        , 0.        , 0.        , 0.05571436,\n",
       "        0.        , 0.05571436, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05571436, 0.05571436,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05571436, 0.        , 0.        , 0.        , 0.05571436,\n",
       "        0.05571436, 0.        , 0.        , 0.05571436, 0.        ,\n",
       "        0.        , 0.05571436, 0.05571436, 0.05571436, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05571436,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05571436, 0.05571436, 0.        , 0.05571436, 0.        ,\n",
       "        0.05571436, 0.        ],\n",
       "       [0.02708311, 0.02708311, 0.02708311, 0.02708311, 0.        ,\n",
       "        0.07917434, 0.        , 0.02708311, 0.02708311, 0.        ,\n",
       "        0.        , 0.02708311, 0.02708311, 0.        , 0.        ,\n",
       "        0.        , 0.02708311, 0.02708311, 0.02708311, 0.02708311,\n",
       "        0.02708311, 0.02708311, 0.02708311, 0.02708311, 0.        ,\n",
       "        0.        , 0.02708311, 0.02708311, 0.        , 0.02708311,\n",
       "        0.02708311, 0.        , 0.        , 0.        , 0.02708311,\n",
       "        0.        , 0.        , 0.        , 0.02708311, 0.02708311,\n",
       "        0.        , 0.02708311, 0.02708311, 0.02708311, 0.02708311,\n",
       "        0.02708311, 0.02708311, 0.        , 0.02708311, 0.02708311,\n",
       "        0.        , 0.02708311, 0.02708311, 0.        , 0.02708311,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08623948, 0.        , 0.        , 0.        , 0.1269016 ,\n",
       "        0.08623948, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08623948, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04398236, 0.04398236, 0.04398236, 0.        , 0.        ,\n",
       "        0.08623948, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08623948, 0.        , 0.        ,\n",
       "        0.        , 0.30995393, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_term_freq: np.ndarray = np.log1p(term_freq)\n",
    "dampened_tf: np.ndarray = log_term_freq / np.sum(log_term_freq, axis=1, keepdims=True)\n",
    "dampened_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 3, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 10,\n",
       "         'and': 5,\n",
       "         'a': 3,\n",
       "         'cat': 3,\n",
       "         'for': 2,\n",
       "         'of': 2,\n",
       "         'sat': 2,\n",
       "         'on': 2,\n",
       "         'dog': 2,\n",
       "         'chased': 2,\n",
       "         'thank': 1,\n",
       "         'you': 1,\n",
       "         'jesus': 1,\n",
       "         'gift': 1,\n",
       "         'today': 1,\n",
       "         'i': 1,\n",
       "         'am': 1,\n",
       "         'grateful': 1,\n",
       "         'it': 1,\n",
       "         'is': 1,\n",
       "         'definitely': 1,\n",
       "         'beautiful': 1,\n",
       "         'day': 1,\n",
       "         'consistency': 1,\n",
       "         'bedrock': 1,\n",
       "         'progress': 1,\n",
       "         'reliability': 1,\n",
       "         'fosters': 1,\n",
       "         'trust': 1,\n",
       "         'builds': 1,\n",
       "         'habits': 1,\n",
       "         'drives': 1,\n",
       "         'success': 1,\n",
       "         'across': 1,\n",
       "         'all': 1,\n",
       "         'endeavors': 1,\n",
       "         'from': 1,\n",
       "         'personal': 1,\n",
       "         'growth': 1,\n",
       "         'to': 1,\n",
       "         'professional': 1,\n",
       "         'achievements': 1,\n",
       "         'enabling': 1,\n",
       "         'steady': 1,\n",
       "         'improvement': 1,\n",
       "         'creating': 1,\n",
       "         'foundation': 1,\n",
       "         'excellence': 1,\n",
       "         'in': 1,\n",
       "         'life': 1,\n",
       "         's': 1,\n",
       "         'myriad': 1,\n",
       "         'pursuits': 1,\n",
       "         'mat': 1,\n",
       "         'log': 1,\n",
       "         'mouse': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "counter: dict = Counter(flatten_documents(corpus))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>ukn</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>2</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>3</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 57)\n",
       "┌─────┬──────────────┬────────┬─────┬───┬───────┬───────┬─────┬─────┐\n",
       "│ a   ┆ achievements ┆ across ┆ all ┆ … ┆ today ┆ trust ┆ you ┆ ukn │\n",
       "│ --- ┆ ---          ┆ ---    ┆ --- ┆   ┆ ---   ┆ ---   ┆ --- ┆ --- │\n",
       "│ i32 ┆ i32          ┆ i32    ┆ i32 ┆   ┆ i32   ┆ i32   ┆ i32 ┆ i32 │\n",
       "╞═════╪══════════════╪════════╪═════╪═══╪═══════╪═══════╪═════╪═════╡\n",
       "│ 2   ┆ 0            ┆ 0      ┆ 0   ┆ … ┆ 1     ┆ 0     ┆ 1   ┆ 0   │\n",
       "│ 1   ┆ 1            ┆ 1      ┆ 1   ┆ … ┆ 0     ┆ 1     ┆ 0   ┆ 0   │\n",
       "│ 0   ┆ 0            ┆ 0      ┆ 0   ┆ … ┆ 0     ┆ 0     ┆ 0   ┆ 0   │\n",
       "└─────┴──────────────┴────────┴─────┴───┴───────┴───────┴─────┴─────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df: pl.DataFrame = pl.DataFrame(freq_count)\n",
    "freq_df.columns = [*vocab.keys()]\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>ukn</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.10857</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.0</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.055714</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.055714</td><td>0.055714</td><td>0.0</td><td>0.055714</td><td>0.0</td><td>0.055714</td><td>0.0</td></tr><tr><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.079174</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.027083</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.027083</td><td>0.027083</td><td>0.0</td><td>0.027083</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.086239</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.126902</td><td>0.086239</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.086239</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.043982</td><td>0.043982</td><td>0.043982</td><td>0.0</td><td>0.0</td><td>0.086239</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.086239</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.309954</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 57)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬─────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ today    ┆ trust    ┆ you      ┆ ukn │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ --- │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64 │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪═════╡\n",
       "│ 0.10857  ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.055714 ┆ 0.0      ┆ 0.055714 ┆ 0.0 │\n",
       "│ 0.027083 ┆ 0.027083     ┆ 0.027083 ┆ 0.027083 ┆ … ┆ 0.0      ┆ 0.027083 ┆ 0.0      ┆ 0.0 │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0      ┆ 0.0 │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴─────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dampened_tf_df: pl.DataFrame = pl.DataFrame(dampened_tf)\n",
    "dampened_tf_df.columns = [*vocab.keys()]\n",
    "dampened_tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "- The inverse document frequency is a `measure of how much information the word provides`, i.e., how common or rare it is across all documents.\n",
    "- It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient)\n",
    "\n",
    "$$ n_{t} = |d \\in D: t \\in d| $$\n",
    "$$ idf_{(t,D)} = log (\\frac{N}{n_{t} + 1}) + 1 $$\n",
    "\n",
    "- where $N$ is the total number of documents in the corpus, $t \\in d$ is the number of terms in a document, $d \\in D$ is a document in the corpus $D$ and 1 is added to the denominator to avoid division-by-zero errors.\n",
    "- $n_{t}$ is the number of documents that contain the term $t$.\n",
    "\n",
    "- If the term is not in the corpus, this will lead to a division-by-zero.\n",
    "  - It is therefore common to adjust the numerator and denominator by adding a smoothing term to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69314718, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.69314718, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.69314718, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.69314718,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 0.55961579, 0.91629073, 0.91629073, 0.91629073,\n",
       "        0.91629073, 1.38629436]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(term_freq != 0)[:, 7].sum()\n",
    "\n",
    "N: int = len(corpus)  # number of documents\n",
    "S_F: int = 1  # smoothing factor\n",
    "\n",
    "# Number of documents containing a term\n",
    "doc_freq: np.ndarray = term_freq != 0\n",
    "doc_freq = doc_freq.sum(axis=0, keepdims=True)\n",
    "\n",
    "idf: np.ndarray = np.log1p((N) / (doc_freq + S_F))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>ukn</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.693147</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.693147</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.693147</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.693147</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.559616</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>0.916291</td><td>1.386294</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 57)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ today    ┆ trust    ┆ you      ┆ ukn      │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0.693147 ┆ 0.916291     ┆ 0.916291 ┆ 0.916291 ┆ … ┆ 0.916291 ┆ 0.916291 ┆ 0.916291 ┆ 1.386294 │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_df: pl.DataFrame = pl.DataFrame(idf)\n",
    "idf_df.columns = [*vocab.keys()]\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>ukn</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.088815</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.045577</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.060249</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.045577</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.036796</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.060249</td><td>0.0</td></tr><tr><td>0.021557</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.063019</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.021557</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.021557</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.017404</td><td>0.028497</td><td>0.0</td><td>0.028497</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.076004</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.147844</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.051241</td><td>0.051241</td><td>0.051241</td><td>0.0</td><td>0.0</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.220542</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 57)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬─────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ today    ┆ trust    ┆ you      ┆ ukn │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ --- │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64 │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪═════╡\n",
       "│ 0.088815 ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.060249 ┆ 0.0      ┆ 0.060249 ┆ 0.0 │\n",
       "│ 0.021557 ┆ 0.028497     ┆ 0.028497 ┆ 0.028497 ┆ … ┆ 0.0      ┆ 0.028497 ┆ 0.0      ┆ 0.0 │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0      ┆ 0.0 │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴─────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf: np.ndarray = dampened_tf * idf\n",
    "# Normalize\n",
    "tf_idf = tf_idf / tf_idf.sum(axis=-1, keepdims=True)\n",
    "\n",
    "tf_idf_df: pl.DataFrame = pl.DataFrame(tf_idf)\n",
    "tf_idf_df.columns = [*vocab.keys()]\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 58)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>ukn</th><th>total</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.088815</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.045577</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.060249</td><td>0.060249</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.045577</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.060249</td><td>0.036796</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>0.060249</td><td>0.0</td><td>1.0</td></tr><tr><td>0.021557</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.063019</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.021557</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.028497</td><td>0.021557</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.028497</td><td>0.028497</td><td>0.0</td><td>0.017404</td><td>0.028497</td><td>0.0</td><td>0.028497</td><td>0.0</td><td>0.0</td><td>1.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.076004</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.147844</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.051241</td><td>0.051241</td><td>0.051241</td><td>0.0</td><td>0.0</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.100472</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.220542</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 58)\n",
       "┌──────────┬──────────────┬──────────┬──────────┬───┬──────────┬──────────┬─────┬───────┐\n",
       "│ a        ┆ achievements ┆ across   ┆ all      ┆ … ┆ trust    ┆ you      ┆ ukn ┆ total │\n",
       "│ ---      ┆ ---          ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ --- ┆ ---   │\n",
       "│ f64      ┆ f64          ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64 ┆ f64   │\n",
       "╞══════════╪══════════════╪══════════╪══════════╪═══╪══════════╪══════════╪═════╪═══════╡\n",
       "│ 0.088815 ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.060249 ┆ 0.0 ┆ 1.0   │\n",
       "│ 0.021557 ┆ 0.028497     ┆ 0.028497 ┆ 0.028497 ┆ … ┆ 0.028497 ┆ 0.0      ┆ 0.0 ┆ 1.0   │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0      ┆ 0.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0 ┆ 1.0   │\n",
       "└──────────┴──────────────┴──────────┴──────────┴───┴──────────┴──────────┴─────┴───────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df.with_columns(total=pl.sum_horizontal(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world from python'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lower(text: str | list[str]) -> str | list[str]:\n",
    "    \"\"\"\n",
    "    Convert input text to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text (str | list[str]): Input text or list of strings to convert.\n",
    "\n",
    "    Returns:\n",
    "        str | list[str]: Lowercase version of input text or list of lowercase strings.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input is not a string or list of strings.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    elif isinstance(text, list):\n",
    "        return [t.lower() if isinstance(t, str) else t for t in text]\n",
    "    else:\n",
    "        raise TypeError(f\"Expected str or list of str, got {type(text).__name__}\")\n",
    "\n",
    "\n",
    "to_lower(\"Hello World from Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    \"\"\"\n",
    "    A class for calculating TF-IDF (Term Frequency-Inverse Document Frequency) scores\n",
    "    for a corpus of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_corpus: list[list[str]], vocab: dict[str, int]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TFIDF class with a corpus and vocabulary.\n",
    "\n",
    "        Args:\n",
    "            corpus (list[list[str]]): A list of documents, where each document is a list of tokenized words.\n",
    "            vocab (dict[str, int]): A dictionary mapping words to their indices in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.corpus: list[list[str]] = tok_corpus\n",
    "        self.vocab: dict[str, int] = vocab\n",
    "\n",
    "    def _calculate_tf(self, dampen: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the term frequency (TF) for each term in each document.\n",
    "\n",
    "        Args:\n",
    "            dampen (bool): Whether to apply logarithmic dampening to the term frequencies.\n",
    "            Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The term frequency matrix with shape (n_docs, n_terms).\n",
    "        \"\"\"\n",
    "        unk_token: str = \"ukn\"\n",
    "\n",
    "        # (n_docs, n_terms)\n",
    "        freq_count: np.ndarray = np.zeros(\n",
    "            (len(self.corpus), len(self.vocab)), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        for idx, doc in enumerate(self.corpus, start=0):\n",
    "            for word in doc:\n",
    "                word_idx: int = self.vocab.get(word, self.vocab[unk_token])\n",
    "                freq_count[idx, word_idx] += 1\n",
    "\n",
    "        if dampen:\n",
    "            # Aggressively dampen the term frequencies.\n",
    "            log_tf: np.ndarray = np.log1p(np.log1p(freq_count))\n",
    "            tf: np.ndarray = log_tf / np.sum(log_tf, axis=1, keepdims=True)\n",
    "        else:\n",
    "            tf = freq_count / np.sum(freq_count, axis=1, keepdims=True)\n",
    "\n",
    "        return tf\n",
    "\n",
    "    def _calculate_idf(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the inverse document frequency (IDF) for each term.\n",
    "        \"\"\"\n",
    "        N: int = len(self.corpus)  # number of documents\n",
    "        S_F: int = 1  # smoothing factor\n",
    "        tf: np.ndarray = self._calculate_tf()\n",
    "\n",
    "        # Number of documents containing a term\n",
    "        doc_freq: np.ndarray = tf > 0\n",
    "        doc_freq: np.ndarray = doc_freq.sum(axis=0, keepdims=True)\n",
    "        idf: np.ndarray = np.log1p(N / (doc_freq + S_F))\n",
    "\n",
    "        return idf\n",
    "\n",
    "    def calculate_tfidf(self, normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the TF-IDF scores for the corpus.\n",
    "        \"\"\"\n",
    "        tf: np.ndarray = self._calculate_tf()\n",
    "        idf: np.ndarray = self._calculate_idf()\n",
    "        tf_idf: np.ndarray = tf * idf\n",
    "        if normalize:\n",
    "            tf_idf = tf_idf / tf_idf.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        return tf_idf\n",
    "\n",
    "\n",
    "def convert_array_to_df(array: np.ndarray, columns: list[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a NumPy array to a Polars DataFrame.\n",
    "\n",
    "    Args:\n",
    "        array (np.ndarray): The input NumPy array to be converted.\n",
    "        columns (list[str]): A list of column names for the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Polars DataFrame with the specified column names.\n",
    "    \"\"\"\n",
    "    data: pl.DataFrame = pl.DataFrame(array)\n",
    "    data.columns = columns\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>achievements</th><th>across</th><th>all</th><th>am</th><th>and</th><th>beautiful</th><th>bedrock</th><th>builds</th><th>cat</th><th>chased</th><th>consistency</th><th>creating</th><th>day</th><th>definitely</th><th>dog</th><th>drives</th><th>enabling</th><th>endeavors</th><th>excellence</th><th>for</th><th>fosters</th><th>foundation</th><th>from</th><th>gift</th><th>grateful</th><th>growth</th><th>habits</th><th>i</th><th>improvement</th><th>in</th><th>is</th><th>it</th><th>jesus</th><th>life</th><th>log</th><th>mat</th><th>mouse</th><th>myriad</th><th>of</th><th>on</th><th>personal</th><th>professional</th><th>progress</th><th>pursuits</th><th>reliability</th><th>s</th><th>sat</th><th>steady</th><th>success</th><th>thank</th><th>the</th><th>to</th><th>today</th><th>trust</th><th>you</th><th>ukn</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.06578</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.061772</td><td>0.0</td><td>0.061772</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.061772</td><td>0.061772</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046729</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.061772</td><td>0.061772</td><td>0.0</td><td>0.0</td><td>0.061772</td><td>0.0</td><td>0.0</td><td>0.061772</td><td>0.061772</td><td>0.061772</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.046729</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.061772</td><td>0.037727</td><td>0.0</td><td>0.061772</td><td>0.0</td><td>0.061772</td><td>0.0</td></tr><tr><td>0.022165</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.036608</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.022165</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0293</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0293</td><td>0.022165</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.0293</td><td>0.0293</td><td>0.0</td><td>0.017895</td><td>0.0293</td><td>0.0</td><td>0.0293</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.08388</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.130099</td><td>0.110883</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.110883</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.078769</td><td>0.078769</td><td>0.078769</td><td>0.0</td><td>0.0</td><td>0.110883</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.110883</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.106182</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 57)\n",
       "┌──────────┬──────────────┬────────┬────────┬───┬──────────┬────────┬──────────┬─────┐\n",
       "│ a        ┆ achievements ┆ across ┆ all    ┆ … ┆ today    ┆ trust  ┆ you      ┆ ukn │\n",
       "│ ---      ┆ ---          ┆ ---    ┆ ---    ┆   ┆ ---      ┆ ---    ┆ ---      ┆ --- │\n",
       "│ f64      ┆ f64          ┆ f64    ┆ f64    ┆   ┆ f64      ┆ f64    ┆ f64      ┆ f64 │\n",
       "╞══════════╪══════════════╪════════╪════════╪═══╪══════════╪════════╪══════════╪═════╡\n",
       "│ 0.06578  ┆ 0.0          ┆ 0.0    ┆ 0.0    ┆ … ┆ 0.061772 ┆ 0.0    ┆ 0.061772 ┆ 0.0 │\n",
       "│ 0.022165 ┆ 0.0293       ┆ 0.0293 ┆ 0.0293 ┆ … ┆ 0.0      ┆ 0.0293 ┆ 0.0      ┆ 0.0 │\n",
       "│ 0.0      ┆ 0.0          ┆ 0.0    ┆ 0.0    ┆ … ┆ 0.0      ┆ 0.0    ┆ 0.0      ┆ 0.0 │\n",
       "└──────────┴──────────────┴────────┴────────┴───┴──────────┴────────┴──────────┴─────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer: TFIDF = TFIDF(tok_corpus=corpus, vocab=vocab)\n",
    "tf_idf: np.ndarray = vectorizer.calculate_tfidf(normalize=True)\n",
    "\n",
    "tf_idf_df_2: pl.DataFrame = pl.DataFrame(tf_idf)\n",
    "tf_idf_df_2.columns = [*vocab.keys()]\n",
    "tf_idf_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying `Dot Product` And `Term Count` Vectors To Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(content: str, term: str) -> float:\n",
    "    \"\"\"\n",
    "    Count the occurrences of a term in the given content.\n",
    "\n",
    "    Args:\n",
    "        content (str): The text content to search in.\n",
    "        term (str): The term to search for.\n",
    "\n",
    "    Returns:\n",
    "        float: The number of occurrences of the term in the content.\n",
    "    \"\"\"\n",
    "    tokenized_content: list[str] = tokenize(content)\n",
    "    term_count: int = tokenized_content.count(term.lower())\n",
    "\n",
    "    return float(term_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_query = ['hi', 'world']\n",
      "query_vector = [1, 1]\n",
      "count_hello = 1.0\n"
     ]
    }
   ],
   "source": [
    "text: str = \"Hello world, hi everyone in the world\"\n",
    "query: str = \"hi world\"\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "print(f\"{tok_query = }\")\n",
    "query_vector: list[int] = [1 for _ in tok_query]\n",
    "print(f\"{query_vector = }\")\n",
    "count_hello: int = term_count(content=text, term=\"hello\")\n",
    "print(f\"{count_hello = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi: 1.0', 'world: 2.0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Hello world, hi everyone in the world\"\n",
    "[f\"{t}: {term_count(content=text, term=t)}\" for t in tok_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_query = ['the', 'cat', 'in', 'the', 'hat']\n",
      "query_vector = [1, 1, 1, 1, 1]\n",
      "doc_vectors = [[5.0, 0.0, 4.0, 5.0, 0.0], [3.0, 2.0, 2.0, 3.0, 2.0], [0.0, 1.0, 2.0, 0.0, 1.0]]\n",
      "\n",
      "doc_scores = [14.0, 12.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "doc1: str = \"\"\"In light of the big reveal in her interview, the interesting\n",
    "          thing is that the person in the wrong probably made a good\n",
    "          decision in the end.\"\"\"\n",
    "doc2: str = \"\"\"My favorite book is the cat in the hat, which is about a crazy\n",
    "          cat in a hat who breaks into a house and creates the craziest\n",
    "          afternoon for two kids.\"\"\"\n",
    "doc3: str = \"\"\"My careless neighbors apparently let a stray cat stay in their\n",
    "          garage unsupervised, which resulted in my favorite hat that I\n",
    "          let them borrow being ruined.\"\"\"\n",
    "\n",
    "query: str = \"the cat in the hat\"\n",
    "\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "# Count vectors\n",
    "query_vector: list[int] = [1 for t in tok_query]\n",
    "print(f\"{tok_query = }\")\n",
    "print(f\"{query_vector = }\")\n",
    "\n",
    "# Count vectors\n",
    "doc_vectors: list[list[float]] = [\n",
    "    [term_count(content=doc, term=tok) for tok in tok_query]\n",
    "    for doc in [doc1, doc2, doc3]\n",
    "]\n",
    "print(f\"{doc_vectors = }\")\n",
    "\n",
    "doc_scores: list[float] = [\n",
    "    np.dot(query_vector, doc_vector) for doc_vector in doc_vectors\n",
    "]\n",
    "print()\n",
    "print(f\"{doc_scores = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "\n",
    "- The current ranking system prioritizes documents with the most keywords, not necessarily the most relevant ones.\n",
    "- This means documents with common words (like \"the\" and \"in\") rank higher than those with all the specific keywords, even if they appear less frequently.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Replace `Term Count` With `Term Frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(\n",
    "    tok_corpus: list[list[str]], vocab: dict[str, int], dampen: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the term frequency (TF) for each term in the corpus.\n",
    "\n",
    "    Args:\n",
    "        tok_corpus (list[list[str]]): A list of tokenized documents.\n",
    "        vocab (dict[str, int]): A dictionary mapping terms to their indices.\n",
    "        dampen (bool, optional): Whether to apply logarithmic dampening. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The term frequency matrix.\n",
    "    \"\"\"\n",
    "    unk_token: str = \"ukn\"\n",
    "    # (n_docs, n_terms)\n",
    "    freq_count: np.ndarray = np.zeros((len(tok_corpus), len(vocab)), dtype=np.int32)\n",
    "\n",
    "    for idx, doc in enumerate(tok_corpus, start=0):\n",
    "        for word in doc:\n",
    "            word_idx: int = vocab.get(word, vocab[unk_token])\n",
    "            freq_count[idx, word_idx] += 1\n",
    "\n",
    "    if dampen:\n",
    "        # Aggressive dampening\n",
    "        log_tf: np.ndarray = np.log1p(np.log1p(freq_count))\n",
    "        tf: np.ndarray = log_tf / np.sum(log_tf, axis=1, keepdims=True)\n",
    "    else:\n",
    "        tf = freq_count / np.sum(freq_count, axis=1, keepdims=True)\n",
    "\n",
    "    return tf\n",
    "\n",
    "\n",
    "def calculate_idf(tok_corpus: list[list[str]], tf: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) for each term.\n",
    "    \"\"\"\n",
    "    N: int = len(tok_corpus)  # number of documents\n",
    "    S_F: int = 1  # smoothing factor\n",
    "\n",
    "    # Number of documents containing a term\n",
    "    doc_freq: np.ndarray = tf > 0\n",
    "    doc_freq: np.ndarray = doc_freq.sum(axis=0, keepdims=True)\n",
    "    idf: np.ndarray = np.log1p(N / (doc_freq + S_F))\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def calculate_tfidf(\n",
    "    tf: np.ndarray, idf: np.ndarray, normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the TF-IDF scores for the corpus.\n",
    "    \"\"\"\n",
    "    tf_idf: np.ndarray = tf * idf\n",
    "    if normalize:\n",
    "        tf_idf = tf_idf / tf_idf.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_corpus = [['my', 'president', 'is', 'black'], ['i', 'love', 'jesus']]\n",
      "Vocab size: 8\n",
      "vocab = {'black': 0, 'i': 1, 'is': 2, 'jesus': 3, 'love': 4, 'my': 5, 'president': 6, 'ukn': 7}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.25, 0.0, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0],\n",
       " [0.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs: list[list[str]] = [\n",
    "    [\"my president is black\"],\n",
    "    [\"i love jesus\"],\n",
    "]\n",
    "\n",
    "tok_corpus: list[list[str]] = [tokenize(doc=doc) for doc in docs]\n",
    "print(f\"{tok_corpus = }\")\n",
    "vocab: dict[str, int] = generate_vocab(docs=docs)\n",
    "print(f\"{vocab = }\")\n",
    "\n",
    "tf: np.ndarray = calculate_tf(tok_corpus=tok_corpus, vocab=vocab, dampen=True)\n",
    "tf.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'black': 0.25,\n",
       "  'i': 0.0,\n",
       "  'is': 0.25,\n",
       "  'jesus': 0.0,\n",
       "  'love': 0.0,\n",
       "  'my': 0.25,\n",
       "  'president': 0.25,\n",
       "  'ukn': 0.0},\n",
       " {'black': 0.0,\n",
       "  'i': 0.3333333333333333,\n",
       "  'is': 0.0,\n",
       "  'jesus': 0.3333333333333333,\n",
       "  'love': 0.3333333333333333,\n",
       "  'my': 0.0,\n",
       "  'president': 0.0,\n",
       "  'ukn': 0.0}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_array_to_df(array=tf, columns=list(vocab.keys())).to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_custom_tf(\n",
    "    tok_corpus: list[list[str]], vocab: dict[str, int], dampen: bool = True\n",
    ") -> list[dict[str, float]]:\n",
    "    tf: np.ndarray = calculate_tf(tok_corpus, vocab, dampen)\n",
    "    tf_list: list[dict[str, float]] = convert_array_to_df(\n",
    "        tf, columns=list(vocab.keys())\n",
    "    ).to_dicts()\n",
    "    return tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_corpus = [['my', 'president', 'is', 'a', 'black', 'man', 'he', 'is', 'also', 'a', 'kenyan', 'man']]\n",
      "Vocab size: 10\n",
      "vocab = {'a': 0, 'also': 1, 'black': 2, 'he': 3, 'is': 4, 'kenyan': 5, 'man': 6, 'my': 7, 'president': 8, 'ukn': 9}\n",
      "tf.tolist() = [[0.13769762363055363, 0.09781785485138984, 0.09781785485138984, 0.09781785485138984, 0.13769762363055363, 0.09781785485138984, 0.13769762363055363, 0.09781785485138984, 0.09781785485138984, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "docs: list[list[str]] = [\n",
    "    [\"my president is a black man. he is also a kenyan man\"],\n",
    "]\n",
    "\n",
    "tok_corpus: list[list[str]] = [tokenize(doc=doc) for doc in docs]\n",
    "print(f\"{tok_corpus = }\")\n",
    "vocab: dict[str, int] = generate_vocab(docs=docs)\n",
    "print(f\"{vocab = }\")\n",
    "\n",
    "tf: np.ndarray = calculate_tf(tok_corpus=tok_corpus, vocab=vocab)\n",
    "print(f\"{tf.tolist() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">[</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">Without dampening</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">]</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'a'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16666666666666666</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'also'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08333333333333333</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'black'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08333333333333333</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'he'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08333333333333333</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'is'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16666666666666666</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'kenyan'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08333333333333333</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'man'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16666666666666666</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'my'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08333333333333333</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'president'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08333333333333333</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'ukn'</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;255;0;0m[\u001b[0m\u001b[38;2;255;0;0mWithout dampening\u001b[0m\u001b[1;38;2;255;0;0m]\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\n",
       "\u001b[1;38;2;255;0;0m[\u001b[0m\u001b[1;38;2;255;0;0m{\u001b[0m\u001b[32m'a'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.16666666666666666\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'also'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.08333333333333333\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'black'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.08333333333333333\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'he'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.08333333333333333\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\n",
       "\u001b[32m'is'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.16666666666666666\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'kenyan'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.08333333333333333\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'man'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.16666666666666666\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'my'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.08333333333333333\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\n",
       "\u001b[32m'president'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.08333333333333333\u001b[0m\u001b[38;2;255;0;0m, \u001b[0m\u001b[32m'ukn'\u001b[0m\u001b[38;2;255;0;0m: \u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;38;2;255;0;0m}\u001b[0m\u001b[1;38;2;255;0;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76ff7b; text-decoration-color: #76ff7b; font-weight: bold\">[</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">With dampening</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b; font-weight: bold\">]</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span>\n",
       "<span style=\"color: #76ff7b; text-decoration-color: #76ff7b; font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'a'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13769762363055363</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'also'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09781785485138984</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'black'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09781785485138984</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'he'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09781785485138984</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'is'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13769762363055363</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'kenyan'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09781785485138984</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'man'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13769762363055363</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'my'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09781785485138984</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'president'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09781785485138984</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'ukn'</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"color: #76ff7b; text-decoration-color: #76ff7b; font-weight: bold\">}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;255;123m[\u001b[0m\u001b[38;2;118;255;123mWith dampening\u001b[0m\u001b[1;38;2;118;255;123m]\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\n",
       "\u001b[1;38;2;118;255;123m[\u001b[0m\u001b[1;38;2;118;255;123m{\u001b[0m\u001b[32m'a'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.13769762363055363\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'also'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.09781785485138984\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'black'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.09781785485138984\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'he'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.09781785485138984\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\n",
       "\u001b[32m'is'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.13769762363055363\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'kenyan'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.09781785485138984\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'man'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.13769762363055363\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'my'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.09781785485138984\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\n",
       "\u001b[32m'president'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.09781785485138984\u001b[0m\u001b[38;2;118;255;123m, \u001b[0m\u001b[32m'ukn'\u001b[0m\u001b[38;2;118;255;123m: \u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;38;2;118;255;123m}\u001b[0m\u001b[1;38;2;118;255;123m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Without dampening\n",
    "res: list[dict[str]] = calculate_custom_tf(\n",
    "    tok_corpus=tok_corpus, vocab=vocab, dampen=False\n",
    ")\n",
    "console.print(f\"[Without dampening]: \\n{res}\", style=\"error\")\n",
    "\n",
    "# With dampening\n",
    "res: list[dict[str]] = calculate_custom_tf(\n",
    "    tok_corpus=tok_corpus, vocab=vocab, dampen=True\n",
    ")\n",
    "console.print(f\"[With dampening]: \\n{res}\", style=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tok_query = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cat'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hat'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tok_query = \u001b[1m[\u001b[0m\u001b[32m'the'\u001b[0m, \u001b[32m'cat'\u001b[0m, \u001b[32m'in'\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'hat'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_vector = [1, 1, 1, 1, 1]\n",
      "tok_corpus = [['in', 'light', 'of', 'the', 'big', 'reveal', 'in', 'her', 'interview', 'the', 'interesting', 'thing', 'is', 'that', 'the', 'person', 'in', 'the', 'wrong', 'probably', 'made', 'a', 'good', 'decision', 'in', 'the', 'end'], ['my', 'favorite', 'book', 'is', 'the', 'cat', 'in', 'the', 'hat', 'which', 'is', 'about', 'a', 'crazy', 'cat', 'in', 'a', 'hat', 'who', 'breaks', 'into', 'a', 'house', 'and', 'creates', 'the', 'craziest', 'afternoon', 'for', 'two', 'kids'], ['my', 'careless', 'neighbors', 'apparently', 'let', 'a', 'stray', 'cat', 'stay', 'in', 'their', 'garage', 'unsupervised', 'which', 'resulted', 'in', 'my', 'favorite', 'hat', 'that', 'i', 'let', 'them', 'borrow', 'being', 'ruined']]\n",
      "Vocab size: 55\n",
      "vocab = {'a': 0, 'about': 1, 'afternoon': 2, 'and': 3, 'apparently': 4, 'being': 5, 'big': 6, 'book': 7, 'borrow': 8, 'breaks': 9, 'careless': 10, 'cat': 11, 'craziest': 12, 'crazy': 13, 'creates': 14, 'decision': 15, 'end': 16, 'favorite': 17, 'for': 18, 'garage': 19, 'good': 20, 'hat': 21, 'her': 22, 'house': 23, 'i': 24, 'in': 25, 'interesting': 26, 'interview': 27, 'into': 28, 'is': 29, 'kids': 30, 'let': 31, 'light': 32, 'made': 33, 'my': 34, 'neighbors': 35, 'of': 36, 'person': 37, 'probably': 38, 'resulted': 39, 'reveal': 40, 'ruined': 41, 'stay': 42, 'stray': 43, 'that': 44, 'the': 45, 'their': 46, 'them': 47, 'thing': 48, 'two': 49, 'unsupervised': 50, 'which': 51, 'who': 52, 'wrong': 53, 'ukn': 54}\n",
      "tf_list = [{'a': 0.045932503967636845, 'about': 0.0, 'afternoon': 0.0, 'and': 0.0, 'apparently': 0.0, 'being': 0.0, 'big': 0.045932503967636845, 'book': 0.0, 'borrow': 0.0, 'breaks': 0.0, 'careless': 0.0, 'cat': 0.0, 'craziest': 0.0, 'crazy': 0.0, 'creates': 0.0, 'decision': 0.045932503967636845, 'end': 0.045932503967636845, 'favorite': 0.0, 'for': 0.0, 'garage': 0.0, 'good': 0.045932503967636845, 'hat': 0.0, 'her': 0.045932503967636845, 'house': 0.0, 'i': 0.0, 'in': 0.08366194876476812, 'interesting': 0.045932503967636845, 'interview': 0.045932503967636845, 'into': 0.0, 'is': 0.045932503967636845, 'kids': 0.0, 'let': 0.0, 'light': 0.045932503967636845, 'made': 0.045932503967636845, 'my': 0.0, 'neighbors': 0.0, 'of': 0.045932503967636845, 'person': 0.045932503967636845, 'probably': 0.045932503967636845, 'resulted': 0.0, 'reveal': 0.045932503967636845, 'ruined': 0.0, 'stay': 0.0, 'stray': 0.0, 'that': 0.045932503967636845, 'the': 0.08955297981776875, 'their': 0.0, 'them': 0.0, 'thing': 0.045932503967636845, 'two': 0.0, 'unsupervised': 0.0, 'which': 0.0, 'who': 0.0, 'wrong': 0.045932503967636845, 'ukn': 0.0}, {'a': 0.063686535901554, 'about': 0.03855930095163874, 'afternoon': 0.03855930095163874, 'and': 0.03855930095163874, 'apparently': 0.0, 'being': 0.0, 'big': 0.0, 'book': 0.03855930095163874, 'borrow': 0.0, 'breaks': 0.03855930095163874, 'careless': 0.0, 'cat': 0.05427970300475833, 'craziest': 0.03855930095163874, 'crazy': 0.03855930095163874, 'creates': 0.03855930095163874, 'decision': 0.0, 'end': 0.0, 'favorite': 0.03855930095163874, 'for': 0.03855930095163874, 'garage': 0.0, 'good': 0.0, 'hat': 0.05427970300475833, 'her': 0.0, 'house': 0.03855930095163874, 'i': 0.0, 'in': 0.05427970300475833, 'interesting': 0.0, 'interview': 0.0, 'into': 0.03855930095163874, 'is': 0.05427970300475833, 'kids': 0.03855930095163874, 'let': 0.0, 'light': 0.0, 'made': 0.0, 'my': 0.03855930095163874, 'neighbors': 0.0, 'of': 0.0, 'person': 0.0, 'probably': 0.0, 'resulted': 0.0, 'reveal': 0.0, 'ruined': 0.0, 'stay': 0.0, 'stray': 0.0, 'that': 0.0, 'the': 0.063686535901554, 'their': 0.0, 'them': 0.0, 'thing': 0.0, 'two': 0.03855930095163874, 'unsupervised': 0.0, 'which': 0.03855930095163874, 'who': 0.03855930095163874, 'wrong': 0.0, 'ukn': 0.0}, {'a': 0.04128293746471496, 'about': 0.0, 'afternoon': 0.0, 'and': 0.0, 'apparently': 0.04128293746471496, 'being': 0.04128293746471496, 'big': 0.0, 'book': 0.0, 'borrow': 0.04128293746471496, 'breaks': 0.0, 'careless': 0.04128293746471496, 'cat': 0.04128293746471496, 'craziest': 0.0, 'crazy': 0.0, 'creates': 0.0, 'decision': 0.0, 'end': 0.0, 'favorite': 0.04128293746471496, 'for': 0.0, 'garage': 0.04128293746471496, 'good': 0.0, 'hat': 0.04128293746471496, 'her': 0.0, 'house': 0.0, 'i': 0.04128293746471496, 'in': 0.058113750235233595, 'interesting': 0.0, 'interview': 0.0, 'into': 0.0, 'is': 0.0, 'kids': 0.0, 'let': 0.058113750235233595, 'light': 0.0, 'made': 0.0, 'my': 0.058113750235233595, 'neighbors': 0.04128293746471496, 'of': 0.0, 'person': 0.0, 'probably': 0.0, 'resulted': 0.04128293746471496, 'reveal': 0.0, 'ruined': 0.04128293746471496, 'stay': 0.04128293746471496, 'stray': 0.04128293746471496, 'that': 0.04128293746471496, 'the': 0.0, 'their': 0.04128293746471496, 'them': 0.04128293746471496, 'thing': 0.0, 'two': 0.0, 'unsupervised': 0.04128293746471496, 'which': 0.04128293746471496, 'who': 0.0, 'wrong': 0.0, 'ukn': 0.0}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tf_vectors = <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08955297981776875</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08366194876476812</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08955297981776875</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.063686535901554</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05427970300475833</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05427970300475833</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.063686535901554</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05427970300475833</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04128293746471496</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058113750235233595</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04128293746471496</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tf_vectors = \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.08955297981776875\u001b[0m, \u001b[1;36m0.0\u001b[0m, \u001b[1;36m0.08366194876476812\u001b[0m, \u001b[1;36m0.08955297981776875\u001b[0m, \u001b[1;36m0.0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m0.063686535901554\u001b[0m, \n",
       "\u001b[1;36m0.05427970300475833\u001b[0m, \u001b[1;36m0.05427970300475833\u001b[0m, \u001b[1;36m0.063686535901554\u001b[0m, \u001b[1;36m0.05427970300475833\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m0.0\u001b[0m, \u001b[1;36m0.04128293746471496\u001b[0m, \n",
       "\u001b[1;36m0.058113750235233595\u001b[0m, \u001b[1;36m0.0\u001b[0m, \u001b[1;36m0.04128293746471496\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">doc_scores = <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2627679084003056</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.290212180817383</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1406796251646635</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "doc_scores = \u001b[1m[\u001b[0m\u001b[1;36m0.2627679084003056\u001b[0m, \u001b[1;36m0.290212180817383\u001b[0m, \u001b[1;36m0.1406796251646635\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc1: str = \"\"\"In light of the big reveal in her interview, the interesting\n",
    "          thing is that the person in the wrong probably made a good\n",
    "          decision in the end.\"\"\"\n",
    "doc2: str = \"\"\"My favorite book is the cat in the hat, which is about a crazy\n",
    "          cat in a hat who breaks into a house and creates the craziest\n",
    "          afternoon for two kids.\"\"\"\n",
    "doc3: str = \"\"\"My careless neighbors apparently let a stray cat stay in their\n",
    "          garage unsupervised, which resulted in my favorite hat that I\n",
    "          let them borrow being ruined.\"\"\"\n",
    "docs: list[list[str]] = [[doc1], [doc2], [doc3]]\n",
    "\n",
    "query: str = \"the cat in the hat\"\n",
    "\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "# Count vectors\n",
    "query_vector: list[int] = [1 for _ in tok_query]\n",
    "console.print(f\"{tok_query = }\")\n",
    "print(f\"{query_vector = }\")\n",
    "\n",
    "tok_corpus: list[list[str]] = [tokenize(doc) for doc in docs]\n",
    "print(f\"{tok_corpus = }\")\n",
    "vocab: dict[str, int] = generate_vocab(docs=docs)\n",
    "print(f\"{vocab = }\")\n",
    "tf_list: list[dict[str, int]] = calculate_custom_tf(\n",
    "    tok_corpus=tok_corpus, vocab=vocab, dampen=True\n",
    ")\n",
    "print(f\"{tf_list = }\")\n",
    "\n",
    "# Term frequency\n",
    "tf_vectors: list[list[float]] = [\n",
    "    [row.get(tok, row[unk_token]) for tok in tok_query] for row in tf_list\n",
    "]\n",
    "console.print(f\"{tf_vectors = }\")\n",
    "\n",
    "doc_scores: list[float] = [np.dot(query_vector, tf_vector) for tf_vector in tf_vectors]\n",
    "print()\n",
    "console.print(f\"{doc_scores = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "#### Comment\n",
    "\n",
    "- The new way of weighting terms (dampened tf) fixed the ranking issue with `\"the\"` and `\"in\"` by giving less importance to frequent words in a single document.\n",
    "- This put the most relevant document (`doc2`) on top, but the improvement wasn't enough to completely overcome the high ranking of `doc1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying TF-IDF To Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_custom_tf_idf(\n",
    "    tf: np.ndarray, idf: np.ndarray, normalize: bool = True\n",
    ") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculate custom TF-IDF scores for given term frequency and inverse document frequency arrays.\n",
    "\n",
    "    Args:\n",
    "        tf (np.ndarray): Term frequency array.\n",
    "        idf (np.ndarray): Inverse document frequency array.\n",
    "        normalize (bool, optional): Whether to normalize the TF-IDF scores. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[dict[str, float]]: A list of dictionaries containing TF-IDF scores for each term.\n",
    "    \"\"\"\n",
    "    tf_idf: np.ndarray = calculate_tfidf(tf, idf, normalize)\n",
    "    tf_idf_list: list[dict[str, float]] = convert_array_to_df(\n",
    "        array=tf_idf, columns=list(vocab.keys())\n",
    "    ).to_dicts()\n",
    "    return tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tok_query = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cat'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hat'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tok_query = \u001b[1m[\u001b[0m\u001b[32m'the'\u001b[0m, \u001b[32m'cat'\u001b[0m, \u001b[32m'in'\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'hat'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_vector = [1, 1, 1, 1, 1]\n",
      "tok_corpus = [['in', 'light', 'of', 'the', 'big', 'reveal', 'in', 'her', 'interview', 'the', 'interesting', 'thing', 'is', 'that', 'the', 'person', 'in', 'the', 'wrong', 'probably', 'made', 'a', 'good', 'decision', 'in', 'the', 'end'], ['my', 'favorite', 'book', 'is', 'the', 'cat', 'in', 'the', 'hat', 'which', 'is', 'about', 'a', 'crazy', 'cat', 'in', 'a', 'hat', 'who', 'breaks', 'into', 'a', 'house', 'and', 'creates', 'the', 'craziest', 'afternoon', 'for', 'two', 'kids'], ['my', 'careless', 'neighbors', 'apparently', 'let', 'a', 'stray', 'cat', 'stay', 'in', 'their', 'garage', 'unsupervised', 'which', 'resulted', 'in', 'my', 'favorite', 'hat', 'that', 'i', 'let', 'them', 'borrow', 'being', 'ruined']]\n",
      "Vocab size: 55\n",
      "vocab = {'a': 0, 'about': 1, 'afternoon': 2, 'and': 3, 'apparently': 4, 'being': 5, 'big': 6, 'book': 7, 'borrow': 8, 'breaks': 9, 'careless': 10, 'cat': 11, 'craziest': 12, 'crazy': 13, 'creates': 14, 'decision': 15, 'end': 16, 'favorite': 17, 'for': 18, 'garage': 19, 'good': 20, 'hat': 21, 'her': 22, 'house': 23, 'i': 24, 'in': 25, 'interesting': 26, 'interview': 27, 'into': 28, 'is': 29, 'kids': 30, 'let': 31, 'light': 32, 'made': 33, 'my': 34, 'neighbors': 35, 'of': 36, 'person': 37, 'probably': 38, 'resulted': 39, 'reveal': 40, 'ruined': 41, 'stay': 42, 'stray': 43, 'that': 44, 'the': 45, 'their': 46, 'them': 47, 'thing': 48, 'two': 49, 'unsupervised': 50, 'which': 51, 'who': 52, 'wrong': 53, 'ukn': 54}\n",
      "tf_idf_list = [{'a': 0.02570455439969602, 'about': 0.0, 'afternoon': 0.0, 'and': 0.0, 'apparently': 0.0, 'being': 0.0, 'big': 0.0420875276773185, 'book': 0.0, 'borrow': 0.0, 'breaks': 0.0, 'careless': 0.0, 'cat': 0.0, 'craziest': 0.0, 'crazy': 0.0, 'creates': 0.0, 'decision': 0.0420875276773185, 'end': 0.0420875276773185, 'favorite': 0.0, 'for': 0.0, 'garage': 0.0, 'good': 0.0420875276773185, 'hat': 0.0, 'her': 0.0420875276773185, 'house': 0.0, 'i': 0.0, 'in': 0.046818547378208675, 'interesting': 0.0420875276773185, 'interview': 0.0420875276773185, 'into': 0.0, 'is': 0.03183798562122598, 'kids': 0.0, 'let': 0.0, 'light': 0.0420875276773185, 'made': 0.0420875276773185, 'my': 0.0, 'neighbors': 0.0, 'of': 0.0420875276773185, 'person': 0.0420875276773185, 'probably': 0.0420875276773185, 'resulted': 0.0, 'reveal': 0.0420875276773185, 'ruined': 0.0, 'stay': 0.0, 'stray': 0.0, 'that': 0.03183798562122598, 'the': 0.06207339547142809, 'their': 0.0, 'them': 0.0, 'thing': 0.0420875276773185, 'two': 0.0, 'unsupervised': 0.0, 'which': 0.0, 'who': 0.0, 'wrong': 0.0420875276773185, 'ukn': 0.0}, {'a': 0.035639990969425724, 'about': 0.03533153008953287, 'afternoon': 0.03533153008953287, 'and': 0.03533153008953287, 'apparently': 0.0, 'being': 0.0, 'big': 0.0, 'book': 0.03533153008953287, 'borrow': 0.0, 'breaks': 0.03533153008953287, 'careless': 0.0, 'cat': 0.03762382309937943, 'craziest': 0.03533153008953287, 'crazy': 0.03533153008953287, 'creates': 0.03533153008953287, 'decision': 0.0, 'end': 0.0, 'favorite': 0.026727270738990807, 'for': 0.03533153008953287, 'garage': 0.0, 'good': 0.0, 'hat': 0.03762382309937943, 'her': 0.0, 'house': 0.03533153008953287, 'i': 0.0, 'in': 0.03037577876590856, 'interesting': 0.0, 'interview': 0.0, 'into': 0.03533153008953287, 'is': 0.03762382309937943, 'kids': 0.03533153008953287, 'let': 0.0, 'light': 0.0, 'made': 0.0, 'my': 0.026727270738990807, 'neighbors': 0.0, 'of': 0.0, 'person': 0.0, 'probably': 0.0, 'resulted': 0.0, 'reveal': 0.0, 'ruined': 0.0, 'stay': 0.0, 'stray': 0.0, 'that': 0.0, 'the': 0.04414414279979189, 'their': 0.0, 'them': 0.0, 'thing': 0.0, 'two': 0.03533153008953287, 'unsupervised': 0.0, 'which': 0.026727270738990807, 'who': 0.03533153008953287, 'wrong': 0.0, 'ukn': 0.0}, {'a': 0.02310258357760524, 'about': 0.0, 'afternoon': 0.0, 'and': 0.0, 'apparently': 0.03782717298345865, 'being': 0.03782717298345865, 'big': 0.0, 'book': 0.0, 'borrow': 0.03782717298345865, 'breaks': 0.0, 'careless': 0.03782717298345865, 'cat': 0.02861515170889971, 'craziest': 0.0, 'crazy': 0.0, 'creates': 0.0, 'decision': 0.0, 'end': 0.0, 'favorite': 0.02861515170889971, 'for': 0.0, 'garage': 0.03782717298345865, 'good': 0.0, 'hat': 0.02861515170889971, 'her': 0.0, 'house': 0.0, 'i': 0.03782717298345865, 'in': 0.032521372127772605, 'interesting': 0.0, 'interview': 0.0, 'into': 0.0, 'is': 0.0, 'kids': 0.0, 'let': 0.053249090734994044, 'light': 0.0, 'made': 0.0, 'my': 0.04028138212731702, 'neighbors': 0.03782717298345865, 'of': 0.0, 'person': 0.0, 'probably': 0.0, 'resulted': 0.03782717298345865, 'reveal': 0.0, 'ruined': 0.03782717298345865, 'stay': 0.03782717298345865, 'stray': 0.03782717298345865, 'that': 0.02861515170889971, 'the': 0.0, 'their': 0.03782717298345865, 'them': 0.03782717298345865, 'thing': 0.0, 'two': 0.0, 'unsupervised': 0.03782717298345865, 'which': 0.02861515170889971, 'who': 0.0, 'wrong': 0.0, 'ukn': 0.0}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tf_idf_vectors = <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08955297981776875</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08366194876476812</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08955297981776875</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.063686535901554</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05427970300475833</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05427970300475833</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.063686535901554</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05427970300475833</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04128293746471496</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058113750235233595</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04128293746471496</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tf_idf_vectors = \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.08955297981776875\u001b[0m, \u001b[1;36m0.0\u001b[0m, \u001b[1;36m0.08366194876476812\u001b[0m, \u001b[1;36m0.08955297981776875\u001b[0m, \u001b[1;36m0.0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m0.063686535901554\u001b[0m, \n",
       "\u001b[1;36m0.05427970300475833\u001b[0m, \u001b[1;36m0.05427970300475833\u001b[0m, \u001b[1;36m0.063686535901554\u001b[0m, \u001b[1;36m0.05427970300475833\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m0.0\u001b[0m, \u001b[1;36m0.04128293746471496\u001b[0m, \n",
       "\u001b[1;36m0.058113750235233595\u001b[0m, \u001b[1;36m0.0\u001b[0m, \u001b[1;36m0.04128293746471496\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">doc_scores = <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2627679084003056</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.290212180817383</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1406796251646635</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "doc_scores = \u001b[1m[\u001b[0m\u001b[1;36m0.2627679084003056\u001b[0m, \u001b[1;36m0.290212180817383\u001b[0m, \u001b[1;36m0.1406796251646635\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query: str = \"the cat in the hat\"\n",
    "\n",
    "\n",
    "tok_query: list[str] = tokenize(query)\n",
    "# Count vectors\n",
    "query_vector: list[int] = [1 for _ in tok_query]\n",
    "console.print(f\"{tok_query = }\")\n",
    "print(f\"{query_vector = }\")\n",
    "\n",
    "tok_corpus: list[list[str]] = [tokenize(doc) for doc in docs]\n",
    "print(f\"{tok_corpus = }\")\n",
    "vocab: dict[str, int] = generate_vocab(docs=docs)\n",
    "print(f\"{vocab = }\")\n",
    "tf: np.ndarray = calculate_tf(tok_corpus=tok_corpus, vocab=vocab, dampen=True)\n",
    "idf: np.ndarray = calculate_idf(tok_corpus=tok_corpus, tf=tf)\n",
    "\n",
    "tf_idf_list: list[dict[str, int]] = calculate_custom_tf_idf(\n",
    "    tf=tf, idf=idf, normalize=False\n",
    ")\n",
    "print(f\"{tf_idf_list = }\")\n",
    "\n",
    "# TF-IDF\n",
    "tf_idf_vectors: list[list[float]] = [\n",
    "    [row.get(tok, row[unk_token]) for tok in tok_query] for row in tf_list\n",
    "]\n",
    "console.print(f\"{tf_idf_vectors = }\")\n",
    "\n",
    "doc_scores: list[float] = [np.dot(query_vector, t_vec) for t_vec in tf_idf_vectors]\n",
    "print()\n",
    "console.print(f\"{doc_scores = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "- In a real life scenario where we have thousands of document, tf-idf will produce a better result than using only the term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311 (ai_search)",
   "language": "python",
   "name": "ai_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
